---
title: "üìâ Regularizaci√≥n con Ridge, Lasso y Validaci√≥n Cruzada"
subtitle: "üî¨ Continuaci√≥n del an√°lisis: Dataset Boston Housing"
author: "üë®‚Äçüè´ Jorge Iv√°n Romero Gelvez"
institute: "üèõÔ∏è Universidad Jorge Tadeo Lozano"
date: "üìÖ Abril 2025"
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
    highlight-style: dracula
    code-line-numbers: true
    code-annotations: hover
    mermaid:
      theme: forest
    transition: fade
    chalkboard: true
    logo: Utadeo70-fondoblanco.png
    toc: true
    toc-title: "Contenido"
    toc-depth: 1
    incremental: true
    scrollable: true
execute:
  warning: false
  message: false
  echo: true
  freeze: false
jupyter: python3
---

# Introducci√≥n al Deep Learning

![Deep Learning](https://miro.medium.com/max/1400/1*3i2uKqjlAj0G9EoqJEpskg.png)

---

## Objetivos de la Clase

- **Comprender** los fundamentos del Deep Learning.
- **Explorar** arquitecturas de redes neuronales.
- **Aplicar** t√©cnicas de entrenamiento y optimizaci√≥n.
- **Relacionar** conceptos estad√≠sticos con Deep Learning.
- **Desarrollar** habilidades pr√°cticas mediante ejemplos de c√≥digo.

---

# Agenda

1. **Introducci√≥n al Deep Learning** (15 mins)
2. **Fundamentos Matem√°ticos** (30 mins)
3. **Arquitecturas de Redes Neuronales** (45 mins)
4. **Entrenamiento y Optimizaci√≥n** (45 mins)
5. **Aplicaciones y Casos de Uso** (30 mins)
6. **Conclusiones y Preguntas** (15 mins)

---

## 1. Introducci√≥n al Deep Learning

### ¬øQu√© es el Deep Learning?

- Subcampo del **aprendizaje autom√°tico** (Machine Learning).
- Utiliza **redes neuronales profundas**.
- Inspirado en el **cerebro humano**.
- Capacidad para **aprender representaciones jer√°rquicas** de datos.

### Historia y Evoluci√≥n

- **Perceptr√≥n** (1950s): Primer modelo de neurona artificial.
- **Redes Multicapa (MLP)** (1980s): Introducci√≥n de capas ocultas.
- **Resurgimiento** con el auge de **Big Data** y **GPUs** (2010s).
- **Transformers** y **modelos preentrenados** (2020s).

### Aplicaciones Actuales

- **Visi√≥n por Computadora**
- **Procesamiento de Lenguaje Natural (NLP)**
- **Reconocimiento de Voz**
- **Juegos y Rob√≥tica**
- **Medicina y Diagn√≥stico**

---

## 2. Fundamentos Matem√°ticos

### √Ålgebra Lineal

- **Vectores y Matrices**
  - Representaci√≥n de datos y par√°metros.
- **Productos Matriciales**
  - Operaciones fundamentales en redes neuronales.
- **Descomposici√≥n en Valores Singulares (SVD)**
  - Reducci√≥n de dimensionalidad y an√°lisis de datos.

### C√°lculo

- **Derivadas y Gradientes**
  - C√°lculo de la tasa de cambio.
- **Regla de la Cadena**
  - C√°lculo de derivadas de funciones compuestas.
- **Optimizaci√≥n Convexa**
  - T√©cnicas para encontrar m√≠nimos globales.

### Probabilidad y Estad√≠stica

- **Distribuciones de Probabilidad**
  - Modelado de incertidumbre.
- **M√°xima Verosimilitud**
  - Estimaci√≥n de par√°metros.
- **Regularizaci√≥n**
  - Prevenci√≥n del sobreajuste.

---

## 3. Arquitecturas de Redes Neuronales

### Perceptr√≥n y Redes Multicapa (MLP)

- **Neuronas Artificiales**
  - Unidad b√°sica de procesamiento.
- **Funciones de Activaci√≥n**
  - Introducci√≥n de no linealidad (ReLU, Sigmoid, Tanh).
- **Arquitectura Feedforward**
  - Conexiones unidireccionales entre capas.

### Redes Convolucionales (CNN)

- **Convoluciones y Pooling**
  - Extracci√≥n de caracter√≠sticas espaciales.
- **Arquitectura de CNN**
  - Capas convolucionales, de pooling y completamente conectadas.
- **Aplicaciones en Visi√≥n por Computadora**

```{.python}
import tensorflow as tf
from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

model.summary()
```

### Redes Recurrentes (RNN) y LSTM

- **Arquitectura de RNN**
  - Manejo de secuencias y dependencias temporales.
- **Problemas de Desvanecimiento del Gradiente**
  - Dificultades en el aprendizaje de dependencias a largo plazo.
- **LSTM y GRU**
  - Soluciones para capturar dependencias a largo plazo.

### Transformers

- **Mecanismo de Atenci√≥n**
  - Captura relaciones globales en los datos.
- **Arquitectura de Transformers**
  - Encoders y Decoders.
- **Aplicaciones en NLP**
  - Traducci√≥n autom√°tica, generaci√≥n de texto, etc.

---

## 4. Entrenamiento y Optimizaci√≥n

### Funci√≥n de P√©rdida

- **Definici√≥n y Ejemplos**
  - Medida de la discrepancia entre predicci√≥n y realidad.
- **Cross-Entropy, MSE**
  - Usadas en clasificaci√≥n y regresi√≥n, respectivamente.

### Algoritmos de Optimizaci√≥n

- **Gradiente Descendente Estoc√°stico (SGD)**
  - Actualizaci√≥n iterativa de par√°metros.
- **Adam, RMSProp**
  - Optimizaci√≥n adaptativa de tasas de aprendizaje.

### Regularizaci√≥n

- **Dropout**
  - Prevenci√≥n del sobreajuste mediante desconexi√≥n aleatoria de neuronas.
- **Batch Normalization**
  - Aceleraci√≥n del entrenamiento y mejora de la estabilidad.
- **L2 Regularization**
  - Penalizaci√≥n de pesos grandes.

### T√©cnicas Avanzadas

- **Aprendizaje por Transferencia**
  - Reutilizaci√≥n de modelos preentrenados.
- **Data Augmentation**
  - Generaci√≥n de datos sint√©ticos para mejorar la generalizaci√≥n.
- **Early Stopping**
  - Parada temprana del entrenamiento para prevenir el sobreajuste.

---

## 5. Aplicaciones y Casos de Uso

### Visi√≥n por Computadora

- **Clasificaci√≥n de Im√°genes**
  - Identificaci√≥n de objetos en im√°genes.
- **Detecci√≥n de Objetos**
  - Localizaci√≥n y clasificaci√≥n de m√∫ltiples objetos.
- **Segmentaci√≥n Sem√°ntica**
  - Asignaci√≥n de etiquetas a cada p√≠xel de una imagen.

### Procesamiento de Lenguaje Natural (NLP)

- **Modelos de Lenguaje**
  - Predicci√≥n de palabras y generaci√≥n de texto.
- **Traducci√≥n Autom√°tica**
  - Traducci√≥n entre diferentes idiomas.
- **An√°lisis de Sentimientos**
  - Determinaci√≥n de la polaridad de textos.

### Otros Campos

- **Generaci√≥n de Contenido (GANs)**
  - Creaci√≥n de im√°genes, videos y audio sint√©ticos.
- **Sistemas de Recomendaci√≥n**
  - Personalizaci√≥n de contenido para usuarios.
- **Diagn√≥stico M√©dico**
  - Identificaci√≥n de enfermedades a partir de im√°genes y datos cl√≠nicos.

---

## 6. Conclusiones y Preguntas

### Resumen de Puntos Clave

- **Importancia** del Deep Learning en la IA moderna.
- **Principales arquitecturas** y sus aplicaciones.
- **T√©cnicas de entrenamiento** efectivas.
- **Integraci√≥n** de conceptos estad√≠sticos y matem√°ticos.

### Futuro del Deep Learning

- **Investigaci√≥n** en redes m√°s eficientes.
- **√âtica y responsabilidad** en el uso de IA.
- **Integraci√≥n** con otras tecnolog√≠as emergentes.

### Espacio para Preguntas

- **¬øTienes alguna duda?**
- **¬øHay alg√∫n tema que quieras profundizar?**

---

# Recursos Adicionales

- **Libros:**
  - *Deep Learning* by Ian Goodfellow, Yoshua Bengio, and Aaron Courville
  - *An Introduction to Statistical Learning* by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani

- **Cursos en L√≠nea:**
  - Coursera: Deep Learning Specialization
  - fast.ai: Practical Deep Learning for Coders

- **Herramientas:**
  - TensorFlow
  - PyTorch
  - Keras

- **P√°ginas Web:**
  - [arXiv](https://arxiv.org/)
  - [Kaggle](https://www.kaggle.com/)
  - [TensorFlow Documentation](https://www.tensorflow.org/)

---

# ¬°Gracias!

![Gracias](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Thank_you.svg/1024px-Thank_you.svg.png)

---

# Detalles Adicionales

## Introducci√≥n al Deep Learning (15 mins)

- **Breve historia y definici√≥n.**
- **Comparaci√≥n** con el aprendizaje autom√°tico tradicional.
- **Impacto** en la industria y la investigaci√≥n.

## Fundamentos Matem√°ticos (30 mins)

- **√Ålgebra Lineal:** Importancia en la representaci√≥n de datos y operaciones de redes neuronales.
- **C√°lculo:** Optimizaci√≥n de funciones de p√©rdida mediante gradientes.
- **Probabilidad y Estad√≠stica:** Manejo de incertidumbre y generalizaci√≥n.

## Arquitecturas de Redes Neuronales (45 mins)

- **Perceptr√≥n y MLP:** Base de las redes neuronales profundas.
- **CNN:** Detalles de capas convolucionales, aplicaciones en im√°genes.
- **RNN y LSTM:** Manejo de secuencias, aplicaciones en texto y voz.
- **Transformers:** √öltima tendencia en NLP y m√°s all√°.

## Entrenamiento y Optimizaci√≥n (45 mins)

- **Funciones de p√©rdida:** Seg√∫n el problema a resolver.
- **Algoritmos de optimizaci√≥n:** Comparaci√≥n y selecci√≥n.
- **Regularizaci√≥n:** T√©cnicas para prevenir el sobreajuste.
- **Estrategias avanzadas:** Mejora del rendimiento del modelo.

## Aplicaciones y Casos de Uso (30 mins)

- **Ejemplos concretos** de aplicaciones en distintos campos.
- **An√°lisis de casos de estudio exitosos.**
- **Desaf√≠os actuales y futuros** en la aplicaci√≥n de Deep Learning.

## Conclusiones y Preguntas (15 mins)

- **Recapitulaci√≥n** de los temas tratados.
- **Reflexi√≥n** sobre el estado actual y futuro del Deep Learning.
- **Espacio abierto** para resolver dudas y discutir temas de inter√©s.

---

# Notas para el Instructor

- **Tiempo por secci√≥n:** Ajusta el ritmo seg√∫n la interacci√≥n de los participantes.
- **Interactividad:** Incluye preguntas r√°pidas o peque√±os ejercicios para mantener la atenci√≥n.
- **Ejemplos Pr√°cticos:** Incorpora m√°s c√≥digo o demostraciones en vivo si el tiempo lo permite.
- **Recursos Visuales:** Utiliza diagramas y gr√°ficos para explicar arquitecturas y conceptos complejos.
- **Referencias:** Anima a los estudiantes a consultar los libros mencionados para profundizar.

---

# C√≥digo de Ejemplo Adicional

## Entrenamiento de una Red Neuronal Simple con Keras

```{.python}
import tensorflow as tf
from tensorflow.keras import datasets, layers, models

# Cargar y preprocesar datos
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()
train_images, test_images = train_images / 255.0, test_images / 255.0

# Definir la arquitectura del modelo
model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# Compilar el modelo
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Entrenar el modelo
model.fit(train_images, train_labels, epochs=10, 
          validation_data=(test_images, test_labels))

# Evaluar el modelo
test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
print(f'\nPrecisi√≥n en el conjunto de prueba: {test_acc}')
```

## Visualizaci√≥n de Resultados

```{.python}
import matplotlib.pyplot as plt
import numpy as np

# Predecir etiquetas
predictions = model.predict(test_images)

# Funci√≥n para graficar la imagen y la predicci√≥n
def plot_image(i, predictions_array, true_label, img):
    true_label, img = true_label[i][0], img[i]
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])

    plt.imshow(img, cmap=plt.cm.binary)

    predicted_label = np.argmax(predictions_array)
    if predicted_label == true_label:
        color = 'blue'
    else:
        color = 'red'

    plt.xlabel(f"{class_names[predicted_label]} ({100*np.max(predictions_array):2.0f}%)", color=color)

# Asumiendo que tienes una lista de nombres de clases
class_names = ['Avi√≥n', 'Autom√≥vil', 'P√°jaro', 'Gato', 'Ciervo',
               'Perro', 'Rana', 'Caballo', 'Barco', 'Cami√≥n']

# Graficar las primeras 5 im√°genes de prueba
plt.figure(figsize=(10,10))
for i in range(5):
    plt.subplot(1,5,i+1)
    plot_image(i, predictions[i], test_labels, test_images)
plt.show()
```

---

# Referencias

- **Goodfellow, Ian**, Yoshua Bengio, and Aaron Courville. *Deep Learning*. MIT Press, 2016.
- **James, Gareth**, Daniela Witten, Trevor Hastie, and Robert Tibshirani. *An Introduction to Statistical Learning*. Springer, 2023.
- **Bishop, Christopher M.** *Pattern Recognition and Machine Learning*. Springer, 2006.
- **Haykin, Simon.** *Neural Networks and Learning Machines*. Pearson, 2009.
- **LeCun, Yann**, Yoshua Bengio, and Geoffrey Hinton. "Deep Learning." *Nature*, 2015.

---

# Contacto

- **Jorge Romero**
- **Universidad Jorge Tadeo Lozano**
- **Correo Electr√≥nico:** jorge.romero@tadeo.edu.co
- **LinkedIn:** [linkedin.com/in/jorgeromero](https://www.linkedin.com/in/jorgeromero)
- **GitHub:** [github.com/jorgeromero](https://github.com/jorgeromero)

---

# Agradecimientos

- A los autores de *Deep Learning* y *Introduction to Statistical Learning* por sus contribuciones al campo.
- A la comunidad de c√≥digo abierto por proporcionar herramientas poderosas como TensorFlow y PyTorch.
- A mis estudiantes y colegas de la Universidad Jorge Tadeo Lozano por su continuo apoyo y colaboraci√≥n.

---

# Evaluaci√≥n

Por favor, toma unos minutos para completar la evaluaci√≥n de esta clase. Tu retroalimentaci√≥n es invaluable para mejorar futuras sesiones.

- **¬øQu√© te ha parecido la clase?** (Excelente, Bueno, Regular, Malo)
- **¬øQu√© temas te gustar√≠a que se profundicen m√°s?**
- **Comentarios adicionales:**

---

# Pr√≥ximos Pasos

- **Explora** los recursos adicionales proporcionados.
- **Pr√°ctica** con proyectos personales utilizando las herramientas vistas.
- **Participa** en comunidades y foros para mantenerte actualizado.
- **Contin√∫a** tu aprendizaje con cursos avanzados y especializaciones.

---

# ¬°Buena Suerte!

![Buena Suerte](https://media.giphy.com/media/3o7aD2saalBwwftBIY/giphy.gif)

--
# Introducci√≥n al Deep Learning

* El Deep Learning es un subcampo del Machine Learning que utiliza redes neuronales profundas para aprender representaciones jer√°rquicas de los datos.
* Se inspira en el funcionamiento del cerebro humano.
* Es particularmente √∫til en tareas de clasificaci√≥n, regresi√≥n, visi√≥n por computadora y procesamiento de lenguaje natural.

## Objetivos de la Presentaci√≥n

* Comprender los principios b√°sicos del Deep Learning.
* Implementar un modelo de clasificaci√≥n para el conjunto de datos **Breast Cancer** usando **TensorFlow** y **Scikit-Learn**.
* Comparar los resultados obtenidos en t√©rminos de precisi√≥n, sensibilidad y especificidad.

---

# Fundamentos Matem√°ticos

## Definici√≥n del Problema

* El objetivo es clasificar muestras de tejido en "Maligno" o "Benigno".
* El conjunto de datos de **Breast Cancer** contiene caracter√≠sticas derivadas de im√°genes digitales de biopsias.
* Cada muestra tiene 30 atributos num√©ricos.

## Notaci√≥n Matem√°tica

* Sea \$X \in \mathbb{R}^{n \times m}\$ el conjunto de datos, donde \$n\$ es el n√∫mero de muestras y \$m\$ es el n√∫mero de atributos.
* La predicci√≥n del modelo se denota como \$\hat{y} = f(X)\$, donde \$f\$ es la funci√≥n de clasificaci√≥n.

---

# Implementaci√≥n en TensorFlow

```{python}
#| code-line-numbers: "|6|9"
# Importar librer√≠as necesarias
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

# Cargar datos
data = load_breast_cancer()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Definici√≥n del modelo
model = Sequential([
    Dense(16, activation='relu', input_shape=(30,)),
    Dense(8, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Entrenamiento
model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)
```

---

# Implementaci√≥n en Scikit-Learn

```{python}
#| code-line-numbers: "|3|8"
# Importar librer√≠as necesarias
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Definici√≥n y entrenamiento del modelo
clf = LogisticRegression(max_iter=1000)
clf.fit(X_train, y_train)

# Evaluaci√≥n
y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))
```

---

# Comparaci√≥n de Resultados

* **TensorFlow**:

  * Entrenamiento con redes neuronales profundas.
  * Optimizaci√≥n con `adam` y funci√≥n de p√©rdida `binary_crossentropy`.
  * Mejora progresiva en cada √©poca.

* **Scikit-Learn**:

  * Modelo tradicional de regresi√≥n log√≠stica.
  * Optimizaci√≥n con descenso de gradiente.
  * R√°pida convergencia.

---

# Conclusiones

* El modelo de TensorFlow muestra un aprendizaje m√°s profundo y capacidad de generalizaci√≥n.
* Scikit-Learn es m√°s r√°pido de entrenar, pero tiene limitaciones en cuanto a la complejidad de los patrones que puede capturar.
* Ambas t√©cnicas son v√°lidas y su uso depende del contexto del problema y los recursos disponibles.

---

# Preguntas y Discusi√≥n

* ¬øQu√© situaciones consideras m√°s adecuadas para usar TensorFlow sobre Scikit-Learn?
* ¬øC√≥mo podr√≠a mejorarse esta implementaci√≥n?
* ¬øTe gustar√≠a explorar una red convolucional (CNN) para este mismo conjunto de datos?
