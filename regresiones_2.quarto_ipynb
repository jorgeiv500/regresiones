{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"📉 Ridge, Lasso y Validación Cruzada\"\n",
        "subtitle: \"🔧 Mitigando el sobreajuste en el dataset Boston Housing\"\n",
        "author: \"👨‍🏫 Jorge Iván Romero Gelvez\"\n",
        "institute: \"🏛️ Universidad Jorge Tadeo Lozano\"\n",
        "date: \"📅 Abril 2025\"\n",
        "format:\n",
        "  revealjs:\n",
        "    theme: [default, custom.scss]\n",
        "    slide-number: true\n",
        "    highlight-style: dracula\n",
        "    code-line-numbers: true\n",
        "    code-annotations: hover\n",
        "    transition: fade\n",
        "    toc: true\n",
        "    toc-title: \"Contenido\"\n",
        "    toc-depth: 1\n",
        "    incremental: true\n",
        "    scrollable: true\n",
        "execute:\n",
        "  echo: true\n",
        "  warning: false\n",
        "  message: false\n",
        "  freeze: false\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "## ⚙️ Preparación del entorno\n"
      ],
      "id": "71eae608"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cargar dataset Boston Housing\n",
        "url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]\n",
        "\n",
        "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM',\n",
        "           'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
        "\n",
        "X = pd.DataFrame(data, columns=columns)\n",
        "y = target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "id": "5a209db6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 🧮 Regresión Ridge\n",
        "\n",
        "La regresión Ridge añade una penalización L2 a la función de costo:\n",
        "\n",
        "$$\n",
        "\\min_\\beta \\left[ \\sum_{i=1}^n (y_i - X_i \\beta)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 \\right]\n",
        "$$\n",
        "\n",
        "- Reduce coeficientes grandes\n",
        "- Mantiene todas las variables\n",
        "- Ideal para multicolinealidad\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 Entrenamiento y evaluación de Ridge\n"
      ],
      "id": "03c5d4db"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge.predict(X_test)\n",
        "\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
        "\n",
        "r2_ridge, rmse_ridge"
      ],
      "id": "591c80d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 🧹 Regresión Lasso\n",
        "\n",
        "La regresión Lasso añade penalización L1:\n",
        "\n",
        "$$\n",
        "\\min_\\beta \\left[ \\sum_{i=1}^n (y_i - X_i \\beta)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| \\right]\n",
        "$$\n",
        "\n",
        "- Elimina coeficientes irrelevantes\n",
        "- Modelo más simple e interpretable\n",
        "- Ideal para selección de variables\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 Entrenamiento y evaluación de Lasso\n"
      ],
      "id": "ca9998f9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_train, y_train)\n",
        "y_pred_lasso = lasso.predict(X_test)\n",
        "\n",
        "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
        "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
        "\n",
        "r2_lasso, rmse_lasso"
      ],
      "id": "86ed7cc2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 🔁 Validación Cruzada\n",
        "\n",
        "Comparamos la capacidad de generalización de Ridge y Lasso usando 10-fold cross-validation:\n"
      ],
      "id": "eefdb047"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ridge_cv = cross_val_score(Ridge(alpha=1.0), X, y, cv=10, scoring='r2')\n",
        "lasso_cv = cross_val_score(Lasso(alpha=0.1), X, y, cv=10, scoring='r2')\n",
        "\n",
        "ridge_cv.mean(), lasso_cv.mean()"
      ],
      "id": "afba479b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 📊 Comparación de coeficientes\n"
      ],
      "id": "9e7d77c3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(X.columns, ridge.coef_, alpha=0.7, label=\"Ridge\")\n",
        "plt.bar(X.columns, lasso.coef_, alpha=0.7, label=\"Lasso\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Coeficientes: Ridge vs Lasso\")\n",
        "plt.ylabel(\"Valor del Coeficiente\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "394dd64a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 📌 Conclusión final de los resultados obtenidos\n"
      ],
      "id": "22e91c6d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Suponiendo que ya tienes calculados estos valores:\n",
        "# r2_ridge, rmse_ridge, ridge_cv\n",
        "# r2_lasso, rmse_lasso, lasso_cv\n",
        "\n",
        "# Crear el DataFrame resumen\n",
        "resultados_df = pd.DataFrame({\n",
        "    'Modelo': ['Ridge', 'Lasso'],\n",
        "    'R²': [r2_ridge, r2_lasso],\n",
        "    'RMSE': [rmse_ridge, rmse_lasso],\n",
        "    'R² CV promedio': [ridge_cv.mean(), lasso_cv.mean()]\n",
        "})\n",
        "\n",
        "# Mostrar el DataFrame con dos decimales\n",
        "resultados_df = resultados_df.round(2)\n",
        "resultados_df"
      ],
      "id": "bff9bfd0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 🧠 Interpretación\n",
        "\n",
        "- 📉 El modelo lineal original mostró signos de **sobreajuste**.\n",
        "- 🛠️ Ridge mantuvo todas las variables, redujo varianza y estabilizó coeficientes.\n",
        "- 🧹 Lasso eliminó coeficientes irrelevantes, simplificando el modelo.\n",
        "- 🔄 La validación cruzada mostró que ambos métodos **generalizan mejor**.\n",
        "\n",
        "> ✅ Ridge y Lasso son herramientas clave para construir modelos más robustos y útiles en producción.\n",
        "\n",
        "---\n",
        "\n",
        "## 📘  Números y estructuras matemáticas\n",
        "\n",
        "## 🔹 Escalares\n",
        "- Un **escalar** es un único número.\n",
        "- Representa una cantidad en una dimensión.\n",
        "- Ejemplo: `x = 5`\n",
        "\n",
        "## 🔹 Vectores\n",
        "- Lista ordenada de números (componentes).\n",
        "- Dirección y magnitud:\n",
        "\n",
        "$\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\dots \\\\ x_n \\end{bmatrix}$\n",
        "\n",
        "## 🔹 Matrices\n",
        "- Tabla bidimensional de números:\n",
        "\n",
        "$ A = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}$\n",
        "\n",
        "## 🔹 Tensors\n",
        "- Generalización a múltiples dimensiones.\n",
        "- Ej. imágenes RGB, secuencias, videos, etc.\n",
        "\n",
        "---\n",
        "\n",
        "# 📘 Probabilidad y distribuciones\n",
        "\n",
        "## 🔹 Fundamentos\n",
        "- **Probabilidad** mide incertidumbre.\n",
        "- **Variable aleatoria**: asigna valores a eventos.\n",
        "\n",
        "## 🔹 Distribuciones comunes\n",
        "- **Bernoulli**, **Binomial**, **Normal**:\n",
        "\n",
        "\n",
        "$\\mathcal{N}(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)$\n",
        "\n",
        "\n",
        "![Distribuciones comunes](distr.png)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 📘 Teoría de la información\n",
        "\n",
        "## 🔹 Entropía\n",
        "\n",
        "$H(X) = -\\sum_{x} P(x) \\log P(x)$\n",
        "\n",
        "## 🔹 Entropía cruzada\n",
        "\n",
        "$H(P, Q) = -\\sum_x P(x) \\log Q(x)$\n",
        "\n",
        "## 🔹 Divergencia KL\n",
        "\n",
        "$D_{KL}(P \\| Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}\n",
        "$\n",
        "\n",
        "## 🔹 Información mutua\n",
        "\n",
        "$I(X; Y) = H(X) - H(X | Y)\n",
        "$\n",
        "\n",
        "---\n",
        "\n",
        "# 📘 Parámetros y optimización\n",
        "\n",
        "## 🔹 Parámetros vs Hiperparámetros\n",
        "- **Parámetros**: aprendidos durante entrenamiento (ej. pesos  $W$ ).\n",
        "- **Hiperparámetros**: definidos antes (ej. tasa de aprendizaje). Ejemplo, en ridge y lasso, $\\lambda$\n",
        "\n",
        "## 🔹 Optimización\n",
        "- Minimizar la función de pérdida.\n",
        "\n",
        "### Métodos:\n",
        "- **Gradiente descendiente**:\n",
        "\n",
        "$\n",
        "\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta J(\\theta)\n",
        "$\n",
        "\n",
        "- **SGD**, **Adam**, **RMSProp**\n",
        "\n",
        "---\n",
        "\n",
        "# ¡Gracias por tu atención! 🙌\n",
        "\n",
        "- Material basado en *Deep Learning* — Goodfellow, Bengio y Courville.\n",
        "- Presentación para uso docente.\n",
        "\n",
        "---\n"
      ],
      "id": "439119d6"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\jorgei.romerog\\AppData\\Local\\anaconda3\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}