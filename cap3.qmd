---
title: "üìò Capacidad, Sobreajuste y Subajuste"
subtitle: "üîç Exploraci√≥n did√°ctica y aplicada"
author: "üë®‚Äçüè´ Jorge Iv√°n Romero Gelvez"
institute: "üè© Universidad Jorge Tadeo Lozano"
date: "üóïÔ∏è Abril 2025"
format: 
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
    highlight-style: dracula
    code-line-numbers: true
    code-annotations: hover
    mermaid:
      theme: forest
    transition: fade
    chalkboard: true
    logo: Utadeo70-fondoblanco.png
    toc: true
    toc-title: "Contenido"
    toc-depth: 1
    incremental: true
    scrollable: true
execute: 
  warning: false
  message: false
  echo: true
  freeze: false
jupyter: python3
---

## üéì Introducci√≥n

### Contexto hist√≥rico

Desde los a√±os 50, el aprendizaje autom√°tico se ha orientado a que las m√°quinas extraigan patrones desde datos sin instrucciones expl√≠citas. Sin embargo, un reto constante ha sido lograr **modelos que no solo funcionen bien con los datos de entrenamiento, sino que generalicen a datos nuevos**.

### ¬øPor qu√© es importante?

- La generalizaci√≥n es el coraz√≥n del aprendizaje autom√°tico.
- Un modelo poco complejo puede pasar por alto relaciones clave ‚Üí subajuste.
- Un modelo excesivamente complejo puede memorizar datos ‚Üí sobreajuste.
- Balancear esta capacidad es esencial para la eficiencia predictiva.

---

## üñêÔ∏è ¬øQu√© es la Capacidad de un Modelo?

La **capacidad** mide qu√© tan bien un modelo puede ajustar una amplia variedad de funciones. Es un concepto central para entender la relaci√≥n entre el aprendizaje y la generalizaci√≥n.

- üîΩ **Capacidad baja** ‚Üí subajuste: el modelo no logra reducir ni el error de entrenamiento.
- üîº **Capacidad alta** ‚Üí sobreajuste: el modelo memoriza ruido y pierde generalizaci√≥n.

---

## ‚öñÔ∏è Subajuste vs. Sobreajuste

- **Subajuste (Underfitting)**: ocurre cuando el modelo es demasiado simple y no capta ni los patrones del entrenamiento.
- **Sobreajuste (Overfitting)**: sucede cuando el modelo es tan complejo que adapta incluso el ruido o los errores del conjunto de entrenamiento.

> üß† **Analog√≠a**:  
> - Subajuste: dibujar una l√≠nea recta para representar una par√°bola.  
> - Sobreajuste: trazar una curva que pasa exactamente por todos los puntos, incluyendo errores o valores at√≠picos.

---

## üìä Error de Generalizaci√≥n

El **error de generalizaci√≥n** mide cu√°n bien funciona un modelo con datos que nunca ha visto:

$$
\mathbb{E}_{(x,y) \sim p_{\text{data}}} \left[ \mathcal{L}(f(x; \theta), y) \right]
$$

- $\mathcal{L}$: funci√≥n de p√©rdida, como el error cuadr√°tico medio.
- $f(x; \theta)$: modelo con par√°metros $\theta$.
- $p_{\text{data}}$: distribuci√≥n real de los datos.


---

## üß™ Visualizaci√≥n con Modelos Polinomiales

```{python}
#| fig-cap: "Modelos de diferentes capacidades sobre el mismo dataset"
#| code-annotations: hover
#| highlight: [14, 19, 23, 25, 26, 29]

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

np.random.seed(0)
X = np.sort(np.random.rand(15, 1) * 2 - 1, axis=0)
y = 1.5 * X**2 + 0.5 + np.random.normal(0, 0.1, size=X.shape)

degrees = [1, 2, 9]
x_plot = np.linspace(-1, 1, 100).reshape(-1, 1)

plt.figure(figsize=(12, 4))

for i, deg in enumerate(degrees):
    poly = PolynomialFeatures(degree=deg)
    X_poly = poly.fit_transform(X)
    x_plot_poly = poly.transform(x_plot)

    model = LinearRegression().fit(X_poly, y)
    y_pred = model.predict(x_plot_poly)

    plt.subplot(1, 3, i+1)
    plt.scatter(X, y, color='black')
    plt.plot(x_plot, y_pred, label=f"Grado {deg}")
    plt.title(f"Modelo grado {deg}\nMSE: {mean_squared_error(y, model.predict(X_poly)):.2f}")
    plt.xlabel("x")
    plt.ylabel("y")
    plt.legend()

plt.tight_layout()
plt.show()
```

---

## üî¢ Teor√≠a Formal: VC-Dimensi√≥n

La **VC-dimensi√≥n** (Vapnik-Chervonenkis) cuantifica cu√°ntas combinaciones posibles de clasificaci√≥n un modelo puede representar correctamente.

- Si un modelo puede clasificar todas las posibles etiquetas de un conjunto de $d$ puntos sin errores, su VC-dimensi√≥n es al menos $d$.
- Sirve para acotar la diferencia entre error de entrenamiento y generalizaci√≥n:

$$
\text{Error de prueba} \leq \text{Error de entrenamiento} + \text{Complejidad del modelo}
$$

> üîç Cuanto m√°s alta la VC-dimensi√≥n, mayor el riesgo de sobreajuste.

---

## üîΩ El L√≠mite Inferior: Error de Bayes

Incluso el mejor modelo posible tiene un m√≠nimo error:

- Proviene del **ruido inherente** en los datos.
- Tambi√©n de la **aleatoriedad** en la relaci√≥n entre entrada y salida.

Ese l√≠mite es conocido como **Error de Bayes**. Es irreducible.

---

## ‚úÖ Conclusi√≥n

- La **capacidad de un modelo** define su potencia para aprender.
- El objetivo no es minimizar solo el error de entrenamiento, sino **maximizar la generalizaci√≥n**.
- La **regularizaci√≥n**, el **aumento de datos** y la **validaci√≥n cruzada** son aliados clave contra el sobreajuste.

---