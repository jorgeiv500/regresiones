---
title: "ğŸ“˜ Fundamentos de Machine Learning"
subtitle: "ğŸ” CapÃ­tulo  - Ejemplos supervisado y no supervisado"
author: "ğŸ‘¨â€ğŸ« Jorge IvÃ¡n Romero Gelvez"
institute: "ğŸ›ï¸ Universidad Jorge Tadeo Lozano"
date: "ğŸ“… Abril 2025"
format: 
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
    highlight-style: dracula
    code-line-numbers: true
    code-annotations: hover
    mermaid:
      theme: forest
    transition: fade
    chalkboard: true
    logo: Utadeo70-fondoblanco.png
    toc: true
    toc-title: "Contenido"
    toc-depth: 1
    incremental: true
    scrollable: true
execute: 
  warning: false
  message: false
  echo: true
  freeze: false
jupyter: python3
---

## ğŸ“œ Fundamentos: Â¿QuÃ© es el aprendizaje automÃ¡tico?

- El **Machine Learning** busca construir algoritmos que **aprenden patrones a partir de datos**.
- Aprender = mejorar el desempeÃ±o en una tarea con experiencia (datos).

### Tipos principales:

- **Aprendizaje supervisado**: se entrena con pares (entrada, salida deseada).
- **Aprendizaje no supervisado**: el modelo descubre estructura sin etiquetas.

---

## ğŸ¤– Ejemplo 1: Aprendizaje Supervisado - ClasificaciÃ³n

Queremos predecir si un estudiante pasarÃ¡ o no basado en horas de estudio y sueÃ±o.

#### DescripciÃ³n del flujo:
1. Se registran variables relevantes: horas de estudio y sueÃ±o.
2. Se define un modelo de clasificaciÃ³n binaria.
3. Se entrena el modelo con ejemplos etiquetados.
4. Se evalÃºa su rendimiento en datos no vistos.

```{python}
#| code-line-numbers: "|5|9"
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

## Datos: [horas de estudio, horas de sueÃ±o]
X = np.array([[5,6],[1,4],[3,6],[8,7],[1,3],[7,8],[4,4],[6,5]])
y = np.array([1,0,1,1,0,1,0,1])  ## 1 = aprueba, 0 = no aprueba

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)
modelo = LogisticRegression()
modelo.fit(X_train, y_train)
y_pred = modelo.predict(X_test)
print("PrecisiÃ³n:", accuracy_score(y_test, y_pred))
```

::: {.fragment .fade-in}
- **LÃ­nea 5**: Datos simples con 2 caracterÃ­sticas.
- **LÃ­nea 9**: Entrenamos un modelo de regresiÃ³n logÃ­stica.
:::

---

## ğŸ“ˆ Capacidad, underfitting y overfitting

- Un modelo muy simple puede **subajustar** (underfit): no capta patrones.
- Un modelo muy complejo puede **sobreajustar** (overfit): memoriza ruido.

#### Â¿CÃ³mo saber si tengo overfitting?
- PrecisiÃ³n muy alta en entrenamiento y muy baja en prueba.

#### Visual:

```
     Capacidad  â†‘
     Error
      |\_      â† Mejor punto
      |  \_
      |    \______
           â†’
```

---

## ğŸ”§ HiperparÃ¡metros y conjuntos de validaciÃ³n

- **HiperparÃ¡metro**: configuraciÃ³n externa al modelo (ej. $k$ en $k$-vecinos).
- Necesitamos **validaciÃ³n cruzada** o **conjunto de validaciÃ³n** para ajustarlos.

```{python}
#| code-line-numbers: "|6"
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score

modelo = KNeighborsClassifier(n_neighbors=3)
puntajes = cross_val_score(modelo, X, y, cv=4)
print("Puntaje promedio de validaciÃ³n:", puntajes.mean())
```

::: {.fragment .fade-in}
- **LÃ­nea 6**: Aplicamos validaciÃ³n cruzada 4-fold.
:::

---

## ğŸ² Estimadores, sesgo y varianza

- **Estimador**: procedimiento para inferir un valor desconocido.
- **Sesgo**: error por suposiciones demasiado fuertes.
- **Varianza**: sensibilidad a los datos de entrenamiento.

#### Compromiso:
- Alto sesgo â†’ underfitting.
- Alta varianza â†’ overfitting.
- **RegularizaciÃ³n ayuda a encontrar el balance.**

---

## ğŸ“Š Ejemplo 2: Aprendizaje No Supervisado - Clustering

Agruparemos puntos 2D sin etiquetas usando K-Means.

```{python}
#| code-line-numbers: "|7|10"
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

X, _ = make_blobs(n_samples=100, centers=3, cluster_std=1.0, random_state=42)
modelo = KMeans(n_clusters=3, random_state=42)
modelo.fit(X)
labels = modelo.predict(X)

plt.scatter(X[:,0], X[:,1], c=labels, cmap="viridis")
plt.title("Clustering con K-Means")
plt.grid(True)
plt.show()
```

::: {.fragment .fade-in}
- **LÃ­nea 7**: Se crean datos con 3 grupos.
- **LÃ­nea 10**: Se visualiza el agrupamiento sin etiquetas reales.
:::

---

## ğŸ§® EstimaciÃ³n de mÃ¡xima verosimilitud (MLE)

- MÃ©todo clÃ¡sico para estimar parÃ¡metros.
- Se eligen los parÃ¡metros que **maximizan la probabilidad de los datos observados**.

$$
\hat{\theta}_\text{MLE} = \underset{\theta}{\arg\max} \ p(x \mid \theta)
$$

- En muchos modelos de ML (como regresiÃ³n logÃ­stica), se basa en esta idea.

---

## ğŸ“ Enfoque Bayesiano

- A diferencia de MLE, **el enfoque Bayesiano incorpora incertidumbre** sobre los parÃ¡metros.

$$
p(\theta \mid x) = \frac{p(x \mid \theta) p(\theta)}{p(x)}
$$

- Permite actualizar creencias con nuevos datos.
- En la prÃ¡ctica, se usa en inferencia probabilÃ­stica y modelos generativos.

---

## ğŸ§ª Ejercicios resueltos

### 1. ClasificaciÃ³n binaria con datos mÃ©dicos
Dado: presiÃ³n y glucosa â†’ Â¿diabÃ©tico o no?

---

```{python}
X = np.array([[120, 85],[140, 90],[100, 70],[160,100]])
y = np.array([0, 1, 0, 1])
modelo = LogisticRegression().fit(X, y)
modelo.predict([[130, 88]])
```

---{python}
X = np.array([[120, 85],[140, 90],[100, 70],[160,100]])
y = np.array([0, 1, 0, 1])
modelo = LogisticRegression().fit(X, y)
modelo.predict([[130, 88]])
```

### 2. Clustering de clientes por compras

---

```{python}
X = np.array([[100, 2],[300, 10],[200, 5],[400,12]])
modelo = KMeans(n_clusters=2).fit(X)
modelo.labels_
```

---{python}
X = np.array([[100, 2],[300, 10],[200, 5],[400,12]])
modelo = KMeans(n_clusters=2).fit(X)
modelo.labels_
```

### 3. Comparar modelos con validaciÃ³n cruzada

---

```{python}
from sklearn.tree import DecisionTreeClassifier
modelos = [KNeighborsClassifier(), DecisionTreeClassifier()]
for modelo in modelos:
  print(cross_val_score(modelo, X, y, cv=2).mean())
```

---{python}
from sklearn.tree import DecisionTreeClassifier
modelos = [KNeighborsClassifier(), DecisionTreeClassifier()]
for modelo in modelos:
  print(cross_val_score(modelo, X, y, cv=2).mean())
```

---

### 4. Analizar sesgo y varianza
- Entrena modelos con diferentes profundidades (Ã¡rboles).
- Compara precisiÃ³n en entrenamiento vs. validaciÃ³n.

---

## ğŸ ConclusiÃ³n general

- El aprendizaje supervisado y no supervisado son enfoques clave.
- Conceptos como regularizaciÃ³n, validaciÃ³n cruzada y MLE sustentan muchos algoritmos modernos.
- Entender **sesgo y varianza** nos ayuda a diseÃ±ar modelos robustos.
- La perspectiva Bayesiana aÃ±ade un enfoque probabilÃ­stico profundo.
