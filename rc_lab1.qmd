---
title: "📝 Laboratorio 1: Regresión Lineal con California Housing"
author: "👨‍🏫 Jorge Iván Romero Gelvez"
institute: "🏛️ Universidad Jorge Tadeo Lozano"
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
    highlight-style: dracula
    code-line-numbers: true
    code-annotations: hover
    transition: fade
    chalkboard: true
    toc: true
    toc-title: "Contenido"
    toc-depth: 1
    incremental: true
    scrollable: true
execute:
  echo: true
  warning: falseauthor: "👨‍🏫 Jorge Iván Romero Gelvez"
institute: "🏛️ Universidad Jorge Tadeo Lozano"
date: "📅 Abril 2025"
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
    highlight-style: dracula
    code-line-numbers: true
    code-annotations: hover
    transition: fade
    chalkboard: true
    toc: true
    toc-title: "Contenido"
    toc-depth: 1
    incremental: true
    scrollable: true
execute:
  echo: true
  warning: false
  message: false
  freeze: false
jupyter: python3
---gosto  message: false
  freeze: false
jupyter: python3
---

# 📝 Laboratorio 1

## Regresión Lineal con California Housing

En este laboratorio implementaremos **regresión lineal sin regularización**, aplicando:

- **Solución analítica (OLS)**
- **Gradiente Descendiente (GD)**

---

# 📌 Evaluación

- **Peso del laboratorio:** 33% de la nota final
- **Distribución:**
  - Ejercicio 1: Función de costo (11%)
  - Ejercicio 2: Gradiente Descendiente (11%)
  - Ejercicio 3: Comparación GD vs OLS (11%)

---

# 📖 Contenido

1. Librerías  
2. Regresión lineal multivariable  
   - Enunciado del problema  
   - Dataset  
   - Recordatorio  
   - Función de costo  
   - Gradiente Descendiente  
   - Batch GD  
   - Comparación OLS  

---

## 1 - Librerías

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
```

---

## 2.1 - Enunciado del problema

👉 Predecir el **valor medio de las viviendas (MedHouseVal)** en distritos de California usando variables socioeconómicas y geográficas.

---

## 2.2 - Dataset

```python
housing = fetch_california_housing(as_frame=True)
X = housing.data
y = housing.target

print("Dimensiones:", X.shape)
print(X.head())
```

✅ **Resultado esperado:**  
- Dimensiones: `(20640, 8)`  
- Variables: `MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude`  
- Target inicial: `4.52, 3.58, 3.52…`

---

## 2.3 - Recordatorio

Modelo:

$$h_\theta(x) = \theta_0 + \theta_1 x_1 + \dots + \theta_n x_n$$

Función de costo (MSE):

$$J(\theta) = \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2$$

---

## 2.4 - Función de costo

### Ejercicio 1 (11%)

```python
def compute_cost(X, y, beta):
    return np.mean((X @ beta - y)**2)

# Prueba
X_test = np.c_[np.ones(5), np.arange(1,6).reshape(-1,1)]
y_test = np.array([1,2,3,4,5])
beta_test = np.array([0,1])
print(compute_cost(X_test, y_test, beta_test))
```

✅ **Resultado esperado:** `0.0`

---

## 2.5 - Gradiente Descendiente

### Ejercicio 2 (11%)

```python
def gradient_descent(X, y, alpha=0.01, epochs=200):
    m, n = X.shape
    beta = np.zeros(n)
    cost_history = []
    for _ in range(epochs):
        grad = (2/m) * X.T @ (X @ beta - y)
        beta -= alpha * grad
        cost_history.append(compute_cost(X, y, beta))
    return beta, cost_history
```

---

## 2.6 - Aprendiendo parámetros con Batch GD

```python
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = np.c_[np.ones(X_scaled.shape[0]), X_scaled]

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

beta, cost_history = gradient_descent(X_train, y_train.values)
```

---

## 2.7 - Comparación con OLS

### Ejercicio 3 (11%)

```python
y_pred_gd = X_test @ beta
print("GD MSE:", mean_squared_error(y_test, y_pred_gd))
print("GD R²:", r2_score(y_test, y_pred_gd))

ols = LinearRegression()
ols.fit(X_train, y_train)
y_pred_ols = ols.predict(X_test)

print("OLS MSE:", mean_squared_error(y_test, y_pred_ols))
print("OLS R²:", r2_score(y_test, y_pred_ols))
```

✅ **Resultados esperados:**  
- GD MSE: ~0.55 – 0.60  
- GD R²: ~0.60 – 0.61  
- OLS MSE: ~0.55  
- OLS R²: ~0.61

---

## 📊 Gráfica de convergencia

```python
plt.plot(range(1, len(cost_history)+1), cost_history)
plt.xlabel("Iteraciones")
plt.ylabel("MSE")
plt.title("Convergencia del Gradiente Descendiente")
plt.show()
```

---

# 📊 Rúbrica de Evaluación

### Ejercicio 1 — Función de costo (11%)
- Excelente: MSE correcto.
- Aceptable: errores menores.
- Deficiente: incorrecto o incompleto.

### Ejercicio 2 — Gradiente Descendiente (11%)
- Excelente: converge y registra costo.
- Aceptable: curva ruidosa.
- Deficiente: no converge.

### Ejercicio 3 — Comparación con OLS (11%)
- Excelente: métricas correctas y análisis.
- Aceptable: métricas sin comparación.
- Deficiente: métricas incorrectas o incompletas.
