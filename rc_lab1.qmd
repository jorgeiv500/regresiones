---
title: "ğŸ“ Laboratorio 1: RegresiÃ³n Lineal con California Housing"
author: "ğŸ‘¨â€ğŸ« Jorge IvÃ¡n Romero Gelvez"
institute: "ğŸ›ï¸ Universidad Jorge Tadeo Lozano"
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
    highlight-style: dracula
    code-line-numbers: true
    code-annotations: hover
    transition: fade
    chalkboard: true
    toc: true
    toc-title: "Contenido"
    toc-depth: 1
    incremental: true
    scrollable: true
execute:
  echo: true
  warning: falseauthor: "ğŸ‘¨â€ğŸ« Jorge IvÃ¡n Romero Gelvez"
institute: "ğŸ›ï¸ Universidad Jorge Tadeo Lozano"
date: "ğŸ“… Abril 2025"
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
    highlight-style: dracula
    code-line-numbers: true
    code-annotations: hover
    transition: fade
    chalkboard: true
    toc: true
    toc-title: "Contenido"
    toc-depth: 1
    incremental: true
    scrollable: true
execute:
  echo: true
  warning: false
  message: false
  freeze: false
jupyter: python3
---gosto  message: false
  freeze: false
jupyter: python3
---

# ğŸ“ Laboratorio 1

## RegresiÃ³n Lineal con California Housing

En este laboratorio implementaremos **regresiÃ³n lineal sin regularizaciÃ³n**, aplicando:

- **SoluciÃ³n analÃ­tica (OLS)**
- **Gradiente Descendiente (GD)**

---

# ğŸ“Œ EvaluaciÃ³n

- **Peso del laboratorio:** 33% de la nota final
- **DistribuciÃ³n:**
  - Ejercicio 1: FunciÃ³n de costo (11%)
  - Ejercicio 2: Gradiente Descendiente (11%)
  - Ejercicio 3: ComparaciÃ³n GD vs OLS (11%)

---

# ğŸ“– Contenido

1. LibrerÃ­as  
2. RegresiÃ³n lineal multivariable  
   - Enunciado del problema  
   - Dataset  
   - Recordatorio  
   - FunciÃ³n de costo  
   - Gradiente Descendiente  
   - Batch GD  
   - ComparaciÃ³n OLS  

---

## 1 - LibrerÃ­as

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
```

---

## 2.1 - Enunciado del problema

ğŸ‘‰ Predecir el **valor medio de las viviendas (MedHouseVal)** en distritos de California usando variables socioeconÃ³micas y geogrÃ¡ficas.

---

## 2.2 - Dataset

```python
housing = fetch_california_housing(as_frame=True)
X = housing.data
y = housing.target

print("Dimensiones:", X.shape)
print(X.head())
```

âœ… **Resultado esperado:**  
- Dimensiones: `(20640, 8)`  
- Variables: `MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude`  
- Target inicial: `4.52, 3.58, 3.52â€¦`

---

## 2.3 - Recordatorio

Modelo:

$$h_\theta(x) = \theta_0 + \theta_1 x_1 + \dots + \theta_n x_n$$

FunciÃ³n de costo (MSE):

$$J(\theta) = \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2$$

---

## 2.4 - FunciÃ³n de costo

### Ejercicio 1 (11%)

```python
def compute_cost(X, y, beta):
    return np.mean((X @ beta - y)**2)

# Prueba
X_test = np.c_[np.ones(5), np.arange(1,6).reshape(-1,1)]
y_test = np.array([1,2,3,4,5])
beta_test = np.array([0,1])
print(compute_cost(X_test, y_test, beta_test))
```

âœ… **Resultado esperado:** `0.0`

---

## 2.5 - Gradiente Descendiente

### Ejercicio 2 (11%)

```python
def gradient_descent(X, y, alpha=0.01, epochs=200):
    m, n = X.shape
    beta = np.zeros(n)
    cost_history = []
    for _ in range(epochs):
        grad = (2/m) * X.T @ (X @ beta - y)
        beta -= alpha * grad
        cost_history.append(compute_cost(X, y, beta))
    return beta, cost_history
```

---

## 2.6 - Aprendiendo parÃ¡metros con Batch GD

```python
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = np.c_[np.ones(X_scaled.shape[0]), X_scaled]

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

beta, cost_history = gradient_descent(X_train, y_train.values)
```

---

## 2.7 - ComparaciÃ³n con OLS

### Ejercicio 3 (11%)

```python
y_pred_gd = X_test @ beta
print("GD MSE:", mean_squared_error(y_test, y_pred_gd))
print("GD RÂ²:", r2_score(y_test, y_pred_gd))

ols = LinearRegression()
ols.fit(X_train, y_train)
y_pred_ols = ols.predict(X_test)

print("OLS MSE:", mean_squared_error(y_test, y_pred_ols))
print("OLS RÂ²:", r2_score(y_test, y_pred_ols))
```

âœ… **Resultados esperados:**  
- GD MSE: ~0.55 â€“ 0.60  
- GD RÂ²: ~0.60 â€“ 0.61  
- OLS MSE: ~0.55  
- OLS RÂ²: ~0.61

---

## ğŸ“Š GrÃ¡fica de convergencia

```python
plt.plot(range(1, len(cost_history)+1), cost_history)
plt.xlabel("Iteraciones")
plt.ylabel("MSE")
plt.title("Convergencia del Gradiente Descendiente")
plt.show()
```

---

# ğŸ“Š RÃºbrica de EvaluaciÃ³n

### Ejercicio 1 â€” FunciÃ³n de costo (11%)
- Excelente: MSE correcto.
- Aceptable: errores menores.
- Deficiente: incorrecto o incompleto.

### Ejercicio 2 â€” Gradiente Descendiente (11%)
- Excelente: converge y registra costo.
- Aceptable: curva ruidosa.
- Deficiente: no converge.

### Ejercicio 3 â€” ComparaciÃ³n con OLS (11%)
- Excelente: mÃ©tricas correctas y anÃ¡lisis.
- Aceptable: mÃ©tricas sin comparaciÃ³n.
- Deficiente: mÃ©tricas incorrectas o incompletas.
