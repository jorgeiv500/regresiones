---
title: "üìä Regresi√≥n Lineal y Regularizaci√≥n con Ridge y Lasso"
subtitle: "üîç Un an√°lisis con el dataset üèòÔ∏è Boston Housing"
author: "üë®‚Äçüè´ Jorge Iv√°n Romero Gelvez"
institute: "üèõÔ∏è Universidad Jorge Tadeo Lozano"
date: "üìÖ Abril 2025"
format: 
  revealjs: 
    theme: [default, custom.scss]
    slide-number: true
    highlight-style: dracula
    code-line-numbers: true
    code-annotations: hover
    mermaid:
      theme: forest
    transition: fade
    chalkboard: true
    logo: Utadeo70-fondoblanco.png
    toc: true
    toc-title: "Contenido"
    toc-depth: 1
    incremental: true
    scrollable: true
execute: 
  warning: false
  message: false
  echo: true
  freeze: false
jupyter: python3
---
# Introducci√≥n

## üí° ¬øQu√© significa la frase?

> **‚ÄúUn buen modelo no solo ajusta bien los datos, sino que tambi√©n generaliza.‚Äù**

![](1_bt-E2YcPafjiPbZFDMMmNQ.jpeg){alt="Gr√°fico ilustrando generalizaci√≥n en modelos"fig-align="center" width="200" style="border-radius: 1rem;"}
---

## üîß Ajustar bien los datos

- üìâ El modelo **aprende patrones** de los datos de entrenamiento  
- ‚úÖ Tiene bajo error en el conjunto de entrenamiento  
- ‚ùó Pero... puede **memorizar** incluso el ruido  
- ‚ö†Ô∏è Riesgo de **sobreajuste (overfitting)**

---

## Underfitting

### Entreno al modelo con 1 sola raza de perro

![](ch.jpg){alt="Chihuahua" fig-align="center" height="100" style="border-radius: 1rem;"}

- Muestra nueva: ¬øEs perro?
 ![](perro.png){alt="perro" fig-align="center" height="60" style="border-radius: 1rem;"}
- ‚ùå **NO lo reconoce**
- üîé La m√°quina **falla en generalizar** porque no ha visto suficientes ejemplos.

<div style="font-size: 0.9em; color: gray; margin-top: 1em;">
La m√°quina fallar√° en reconocer al perro por falta de suficientes muestras. No puede generalizar el conocimiento.
</div>

---

## Overfitting

### Entreno al modelo con 10 razas de perro color marr√≥n

![](several.png){alt="varios" fig-align="center" heiht="100" style="border-radius: 1rem;"}

- Muestra nueva: ¬øEs perro?
 ![](perro.png){alt="perro" fig-align="center" height="60" style="border-radius: 1rem;"}
- ‚ùå **NO lo reconoce**
- ‚ö†Ô∏è El modelo est√° **demasiado ajustado** a los datos de entrenamiento.

<div style="font-size: 0.9em; color: gray; margin-top: 1em;">
La m√°quina fallar√° en reconocer un perro nuevo porque no tiene estrictamente los mismos valores de las muestras de entrenamiento.
</div>

## üåç Generalizar los datos

- üß† El modelo **funciona bien con datos nuevos**  
- üî¨ Puede **predecir correctamente en el mundo real**  
- üìä Tiene buen rendimiento en el **conjunto de prueba**
- üéØ Es el verdadero objetivo del aprendizaje autom√°tico

---

## üÜö Comparaci√≥n: Ajustar vs. Generalizar

|                      | üîß **Ajustar bien**        | üåç **Generalizar bien**           |
|----------------------|----------------------------|-----------------------------------|
| üéØ Objetivo inicial   | Minimizar error en entrenamiento | Predecir bien datos nuevos         |
| ‚ö†Ô∏è Riesgo             | Sobreajuste (memoriza)     | Subajuste si es demasiado simple  |
| üß∞ Herramientas       | Modelos complejos          | Regularizaci√≥n y validaci√≥n cruzada |

---

## üõ†Ô∏è ¬øC√≥mo logramos un buen equilibrio?

- ‚úÖ Usando **validaci√≥n cruzada**
- ‚öñÔ∏è Aplicando **Ridge y Lasso** para controlar la complejidad
- üß™ Comparando resultados en entrenamiento y prueba

> üéì *El arte del aprendizaje autom√°tico es encontrar el punto justo entre sesgo y varianza.*

![](balance.gif){#fig-surus}

---

## üìå RETOMANDO
- üìö **Modelo base:** punto de partida
- üß∞ **Ridge:** reduce varianza, estabiliza coeficientes
- üßπ **Lasso:** elimina variables irrelevantes
- üîÑ **Validaci√≥n cruzada:** asegura buen desempe√±o fuera de muestra
- üéØ **Objetivo:** lograr un modelo que generalice bien
- üí° El mejor modelo no es el m√°s complejo, sino el que predice con **equilibrio** y **claridad.**

```{mermaid}
%%| echo: false
%%| fig-width: 6.5%%| 
flowchart LR
    A["üì• Dataset limpio (Boston.csv)"] --> B["üìä Modelo base\nRegresi√≥n Lineal"]
    B --> C{"üîç ¬øProblemas?"}
    C -- Sobreajuste / multicolinealidad --> D["üß∞ Ridge (L2)"]
    C -- Muchas variables irrelevantes --> E["üßπ Lasso (L1)"]
    D & E --> F["üß™ Validaci√≥n Cruzada"]
    F --> G["üìà Comparar\nR¬≤ y RMSE"]
    G --> H{"üéØ ¬øBuen desempe√±o?"}
    H -- S√≠ --> I["‚úÖ Seleccionar modelo final"]
    H -- No --> J["‚öôÔ∏è Ajustar hiperpar√°metros\nüîÑ Repetir"]

```

## üÜö Comparaci√≥n de T√©cnicas
::: {.small}
| T√©cnica            | Problema que aborda                                      | C√≥mo lo resuelve                                                       | Ventajas                                                                 | Limitaciones                                                              |
|--------------------|----------------------------------------------------------|-------------------------------------------------------------------------|--------------------------------------------------------------------------|---------------------------------------------------------------------------|
| **Regresi√≥n Ridge**| - Multicolinealidad<br>- Sobreajuste                    | Penalizaci√≥n L2 que reduce el tama√±o de los coeficientes grandes       | - Reduce varianza<br>- Mantiene todas las variables<br>- Buena generalizaci√≥n | - No elimina variables irrelevantes                                       |
| **Regresi√≥n Lasso**| - Multicolinealidad<br>- Sobreajuste<br>- Alta dimensionalidad | Penalizaci√≥n L1 que puede reducir coeficientes a cero (selecci√≥n de variables) | - Selecci√≥n autom√°tica de variables<br>- Modelos m√°s interpretables      | - Puede eliminar variables importantes si est√°n correlacionadas           |
| **Validaci√≥n cruzada** | - Sobreajuste<br>- Mala generalizaci√≥n del modelo | Divide los datos en subconjuntos para evaluar m√∫ltiples veces el modelo | - Estima rendimiento real<br>- Ayuda a elegir hiperpar√°metros √≥ptimos   | - Mayor costo computacional<br>- Sensible a la forma de dividir los datos |
:::
---

# El dataset

---

## 1Ô∏è‚É£ Descripci√≥n del Dataset{.scrollable .smaller}

El conjunto de datos **Boston Housing** contiene informaci√≥n sobre barrios de Boston, recopilada por el U.S. Census.

- Observaciones: 506
- Variables independientes: 13 caracter√≠sticas socioecon√≥micas y urbanas
- Variable dependiente: `MEDV` (Valor medio de vivienda ocupada por sus due√±os, en miles de d√≥lares)
  
## 2Ô∏è‚É£ Descripci√≥n del Dataset{.scrollable .smaller}
| Variable | Descripci√≥n |
|----------|-------------|
| CRIM     | Tasa de criminalidad per c√°pita por ciudad |
| ZN       | Proporci√≥n de terrenos residenciales (>25,000 pies¬≤) |
| INDUS    | Proporci√≥n de tierra para negocios no minoristas |
| CHAS     | Frontera con r√≠o Charles (1: s√≠, 0: no) |
| NOX      | Concentraci√≥n de √≥xidos n√≠tricos (contaminaci√≥n del aire) |
| RM       | N√∫mero promedio de habitaciones por vivienda |
| AGE      | % de unidades construidas antes de 1940 |
| DIS      | Distancia a cinco centros de empleo |
| RAD      | √çndice de accesibilidad a autopistas radiales |
| TAX      | Tasa de impuesto a la propiedad |
| PTRATIO  | Relaci√≥n alumno-profesor en cada barrio |
| B        | Proporci√≥n poblacional afrodescendiente (c√°lculo especial) |
| LSTAT    | % de poblaci√≥n con bajo estatus socioecon√≥mico |
| MEDV     | Valor medio de la vivienda (en miles de d√≥lares) |

---

## Carga del Dataset {.scrollable}

```{python}
import pandas as pd
import numpy as np

# Cargar datos desde CMU
url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM',
           'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']
X = pd.DataFrame(data, columns=columns)
y = pd.Series(target, name='MEDV')
X.head()
```
---

## Divisi√≥n y Modelo

```{python}
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)
```

---
## Evaluaci√≥n del Modelo

```{python}
from sklearn.metrics import r2_score, mean_squared_error

y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

r2_train = r2_score(y_train, y_pred_train)
r2_test = r2_score(y_test, y_pred_test)
mse_train = mean_squared_error(y_train, y_pred_train)
mse_test = mean_squared_error(y_test, y_pred_test)

r2_train, mse_train, r2_test, mse_test
```

---

## An√°lisis Estad√≠stico

```{python}
import statsmodels.api as sm

X_train_sm = sm.add_constant(X_train)
ols_model = sm.OLS(y_train, X_train_sm).fit()
ols_model.summary()
```

---

# Diccionario de Variables

```{python}
descripcion_variables = {
    'CRIM': 'CRIM (Tasa criminalidad per c√°pita)',
    'ZN': 'ZN (% terrenos residenciales > 25,000 pies¬≤)',
    'INDUS': 'INDUS (% tierra para negocios no minoristas)',
    'CHAS': 'CHAS (R√≠o Charles, 1=s√≠)',
    'NOX': 'NOX (Contaminaci√≥n del aire, NOx)',
    'RM': 'RM (Habitaciones promedio por casa)',
    'AGE': 'AGE (% casas construidas antes de 1940)',
    'DIS': 'DIS (Distancia a centros de empleo)',
    'RAD': 'RAD (√çndice de accesibilidad a autopistas)',
    'TAX': 'TAX (Tasa de impuesto a la propiedad)',
    'PTRATIO': 'PTRATIO (Relaci√≥n estudiantes/profesores)',
    'B': 'B (Proporci√≥n poblacional afrodescendiente)',
    'LSTAT': 'LSTAT (% poblaci√≥n de bajo estatus socioecon√≥mico)'
}
```

---

## Gr√°fico de Coeficientes

```{python}
import matplotlib.pyplot as plt

coeficientes = pd.Series(model.coef_, index=X.columns)
coef_desc = coeficientes.rename(index=descripcion_variables)

plt.figure(figsize=(10,6))
coef_desc.sort_values().plot(kind='barh', color='skyblue')
plt.title("Importancia de Variables en Regresi√≥n Lineal")
plt.xlabel("Valor del Coeficiente")
plt.grid(True)
plt.tight_layout()
plt.show()
```

---

## Gr√°ficas de Interpretaci√≥n (Top 4)

```{python}
top_vars = coeficientes.abs().sort_values(ascending=False).head(4).index.tolist()

fig, axs = plt.subplots(2, 2, figsize=(14,10))
axs = axs.flatten()

for i, var in enumerate(top_vars):
    axs[i].scatter(X_test[var], y_test, label='Real', alpha=0.6)
    axs[i].scatter(X_test[var], y_pred_test, label='Predicci√≥n', alpha=0.6)
    axs[i].set_xlabel(descripcion_variables[var])
    axs[i].set_ylabel('MEDV ($1000s)')
    axs[i].set_title(f"{descripcion_variables[var]} vs MEDV")
    axs[i].legend()
    axs[i].grid(True)

plt.suptitle("Variables M√°s Relevantes vs Precio de Vivienda", fontsize=14)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
```

---

# Conclusiones

- La regresi√≥n lineal logra un buen ajuste inicial.
- `RM` y `LSTAT` son las variables m√°s influyentes.
- Los errores son moderados, lo cual sugiere posibilidad de mejorar con regularizaci√≥n.
- El modelo permite interpretar de manera clara c√≥mo cada variable afecta el precio de la vivienda.

---

# Siguientes Pasos

- Explorar modelos regularizados: Ridge, Lasso
- Probar transformaciones no lineales
- Usar validaci√≥n cruzada para robustez
- Aplicar el modelo a nuevos conjuntos de datos