---
title: "ğŸ§  MÃ©tricas y HiperparÃ¡metros en ML"
subtitle: "ğŸ“ˆ EvaluaciÃ³n de Modelos y Ajuste_final"
author: "Jorge IvÃ¡n Romero Gelvez"
lang: es
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
    highlight-style: dracula
    code-line-numbers: true
    code-annotations: hover
    chalkboard: true
    toc: true
    toc-title: "Contenido"
    toc-depth: 1
    incremental: true
    scrollable: true
execute: 
  warning: false
  message: false
  echo: false
  freeze: true
jupyter: python3
---

## ğŸ¯ Â¿QuÃ© es una Matriz de ConfusiÃ³n?

|                      | PredicciÃ³n: Positivo | PredicciÃ³n: Negativo |
|----------------------|----------------------|----------------------|
| **Real: Positivo**   | âœ… TP                | âŒ FN                |
| **Real: Negativo**   | âŒ FP                | âœ… TN                |

- TP: Verdaderos positivos
- TN: Verdaderos negativos
- FP: Falsos positivos
- FN: Falsos negativos

---

## ğŸ“ MÃ©tricas Derivadas

- **Precision** = TP / (TP + FP)
  > Â¿De las predicciones positivas, cuÃ¡ntas fueron correctas?

- **Recall** = TP / (TP + FN)
  > Â¿De los positivos reales, cuÃ¡ntos detectÃ³ correctamente el modelo?

- **F1-score** = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
  > Balance entre precisiÃ³n y recall

- **AUC (Ãrea Bajo la Curva ROC)**
  > Mide la capacidad del modelo para discriminar entre clases.
  > Rango: 0.5 (azar) a 1.0 (perfecto).

---

## ğŸ§  Ejemplo NumÃ©rico

```text
Matriz:
TP = 80  FP = 10
FN = 20  TN = 90
```

- **Precision** = 80 / (80 + 10) = 0.888
- **Recall** = 80 / (80 + 20) = 0.80
- **F1-score** â‰ˆ 0.842
- **AUC** â‰ˆ Ãrea bajo curva ROC generada por TPR vs FPR

---

## ğŸ“Œ Â¿CuÃ¡ndo usar cada mÃ©trica?

| MÃ©trica   | Ãštil cuando...                                      |
|-----------|------------------------------------------------------|
| Precision | FP son costosos (e.g., spam, fraude)                |
| Recall    | FN son peligrosos (e.g., cÃ¡ncer, fallos crÃ­ticos)   |
| F1-score  | Se necesita un balance en clases desbalanceadas     |
| AUC       | Quieres evaluar el poder discriminativo global      |

---

## ğŸ“Œ **Resumen: HiperparÃ¡metros en Aprendizaje AutomÃ¡tico**

- **DefiniciÃ³n**: Configuraciones externas al modelo que controlan su comportamiento. No se aprenden automÃ¡ticamente.

- **Importancia**:
  1. Capacidad de representaciÃ³n del modelo
  2. OptimizaciÃ³n efectiva
  3. RegularizaciÃ³n adecuada

---

## âš™ï¸ Ejemplos de HiperparÃ¡metros

- NÃºmero de capas o neuronas
- Tasa de aprendizaje
- Î» (decaimiento de peso)
- Î± (momento)
- Early stopping
- Maxout, dropout
- Preprocesamientos opcionales

---

## ğŸ§© Capacidad y Curva en U

- ğŸ“‰ **Underfitting**: Capacidad baja â†’ error alto
- ğŸ“ˆ **Overfitting**: Capacidad alta â†’ error de generalizaciÃ³n alto
- âœ… Punto Ã³ptimo: Capacidad intermedia

---

## ğŸš€ Tasa de Aprendizaje

- Es el hiperparÃ¡metro mÃ¡s sensible
- Curva en U del error de entrenamiento
- Muy alta â†’ divergencia
- Muy baja â†’ aprendizaje lento o estancado

---

## ğŸ” SelecciÃ³n y OptimizaciÃ³n

- Usar conjunto de validaciÃ³n (no entrenamiento ni prueba)
- MÃ©todos comunes:
  - ğŸ” Manual
  - ğŸ§® Grid Search
  - ğŸ² Random Search

- Puede verse como un problema de optimizaciÃ³n:
  - Variables: hiperparÃ¡metros
  - FunciÃ³n objetivo: error de validaciÃ³n

---
## ğŸ§® Â¿QuÃ© es Grid Search?

- **Grid Search** es un mÃ©todo de bÃºsqueda exhaustiva para encontrar la mejor combinaciÃ³n de hiperparÃ¡metros.
- Se define una **rejilla (grid)** de posibles valores para cada hiperparÃ¡metro.
- El modelo se entrena y evalÃºa con cada combinaciÃ³n posible.
- Se utiliza una mÃ©trica (e.g., F1, AUC) para comparar los resultados.
- Se puede usar junto a **validaciÃ³n cruzada** para obtener estimaciones mÃ¡s robustas.

Ejemplo de rejilla para regresiÃ³n logÃ­stica con regularizaciÃ³n L2:

```python
param_grid = {
    'logisticregression__C': [0.01, 0.1, 1, 10, 100]
}
```

