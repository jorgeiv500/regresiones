---
title: "ğŸ“ Laboratorio 2: RegresiÃ³n Lineal Regularizada (Ridge y Lasso)"
author: "ğŸ‘¨â€ğŸ« Jorge IvÃ¡n Romero Gelvez"
institute: "ğŸ›ï¸ Universidad Jorge Tadeo Lozano"
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
    highlight-style: dracula
    code-line-numbers: true
    code-annotations: hover
    transition: fade
    chalkboard: true
    toc: true
    toc-title: "Contenido"
    toc-depth: 1
    incremental: true
    scrollable: true
execute:
  echo: true
  warning: false
  message: false
  freeze: false
jupyter: python3
---

# ğŸ“ Laboratorio PrÃ¡ctico

## RegresiÃ³n Lineal Regularizada (Ridge y Lasso)

En este laboratorio implementaremos regresiÃ³n lineal con **regularizaciÃ³n**:

- Ridge Regression (L2)  
- Lasso Regression (L1)

---

# ğŸ“Œ EvaluaciÃ³n

- **Peso del laboratorio:** 33% de la nota final  
- **DistribuciÃ³n:**  
  - Ejercicio 1: Ridge (11%)  
  - Ejercicio 2: Lasso (11%)  
  - Ejercicio 3: ComparaciÃ³n con OLS (11%)  

---

# ğŸ“– Contenido

1. LibrerÃ­as  
2. RegresiÃ³n lineal regularizada  
   - Enunciado del problema  
   - Dataset  
   - Ridge Regression  
   - Lasso Regression  
   - ComparaciÃ³n con OLS  

---

## 1 - LibrerÃ­as

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
````

---

## 2.1 - Enunciado del problema

ğŸ‘‰ Predecir el **valor medio de las viviendas (MEDV)** en distritos de California, aplicando **regularizaciÃ³n** para reducir sobreajuste.

---

## 2.2 - Dataset

```python
housing = fetch_california_housing(as_frame=True)
X = housing.data
y = housing.target

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

print("Dimensiones:", X.shape)
print(X.head())
```

âœ… Resultado esperado: 20,640 muestras, 8 variables predictoras.

---

## 2.3 - Ridge Regression

### Ejercicio 1 (11%)

```python
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

y_pred_ridge = ridge.predict(X_test)

print("Ridge MSE:", mean_squared_error(y_test, y_pred_ridge))
print("Ridge RÂ²:", r2_score(y_test, y_pred_ridge))
```

âœ… Resultado esperado:
MSE \~0.55, RÂ² \~0.61

---

## 2.4 - Lasso Regression

### Ejercicio 2 (11%)

```python
lasso = Lasso(alpha=0.01, max_iter=10000)
lasso.fit(X_train, y_train)

y_pred_lasso = lasso.predict(X_test)

print("Lasso MSE:", mean_squared_error(y_test, y_pred_lasso))
print("Lasso RÂ²:", r2_score(y_test, y_pred_lasso))
print("Coeficientes Lasso:", lasso.coef_)
```

âœ… Resultado esperado:
MSE \~0.57, RÂ² \~0.60
Coeficientes: algunos en 0

---

## 2.5 - ComparaciÃ³n con OLS

### Ejercicio 3 (11%)

```python
ols = LinearRegression()
ols.fit(X_train, y_train)
y_pred_ols = ols.predict(X_test)

resultados = pd.DataFrame({
    "Modelo": ["OLS", "Ridge", "Lasso"],
    "MSE": [
        mean_squared_error(y_test, y_pred_ols),
        mean_squared_error(y_test, y_pred_ridge),
        mean_squared_error(y_test, y_pred_lasso)
    ],
    "RÂ²": [
        r2_score(y_test, y_pred_ols),
        r2_score(y_test, y_pred_ridge),
        r2_score(y_test, y_pred_lasso)
    ]
})

print(resultados)
```

âœ… Resultado esperado:

| Modelo | MSE  | RÂ²   |
| ------ | ---- | ---- |
| OLS    | 0.55 | 0.61 |
| Ridge  | 0.55 | 0.61 |
| Lasso  | 0.57 | 0.60 |

---

# ğŸ“Š RÃºbrica de EvaluaciÃ³n

* **Ejercicio 1 â€” Ridge (11%)**: correcta implementaciÃ³n, MSE y RÂ².
* **Ejercicio 2 â€” Lasso (11%)**: correcta implementaciÃ³n, coeficientes en 0.
* **Ejercicio 3 â€” ComparaciÃ³n (11%)**: tabla comparativa y discusiÃ³n (Ridge reduce magnitud, Lasso elimina variables).

