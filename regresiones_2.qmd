---
title: "ğŸ“‰ Ridge, Lasso y ValidaciÃ³n Cruzada"
subtitle: "ğŸ”§ Mitigando el sobreajuste en el dataset Boston Housing"
author: "ğŸ‘¨â€ğŸ« Jorge IvÃ¡n Romero Gelvez"
institute: "ğŸ›ï¸ Universidad Jorge Tadeo Lozano"
date: "ğŸ“… Abril 2025"
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
    highlight-style: dracula
    code-line-numbers: true
    code-annotations: hover
    transition: fade
    toc: true
    toc-title: "Contenido"
    toc-depth: 1
    incremental: true
    scrollable: true
execute:
  echo: true
  warning: false
  message: false
  freeze: false
jupyter: python3
---

## âš™ï¸ PreparaciÃ³n del entorno

```{python}
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt

# Cargar dataset Boston Housing
url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM',
           'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']

X = pd.DataFrame(data, columns=columns)
y = target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

---

## ğŸ§® RegresiÃ³n Ridge

La regresiÃ³n Ridge aÃ±ade una penalizaciÃ³n L2 a la funciÃ³n de costo:

$$
\min_\beta \left[ \sum_{i=1}^n (y_i - X_i \beta)^2 + \lambda \sum_{j=1}^p \beta_j^2 \right]
$$

- Reduce coeficientes grandes
- Mantiene todas las variables
- Ideal para multicolinealidad

---

## ğŸ”§ Entrenamiento y evaluaciÃ³n de Ridge

```{python}
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
y_pred_ridge = ridge.predict(X_test)

r2_ridge = r2_score(y_test, y_pred_ridge)
rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))

r2_ridge, rmse_ridge
```

---

## ğŸ§¹ RegresiÃ³n Lasso

La regresiÃ³n Lasso aÃ±ade penalizaciÃ³n L1:

$$
\min_\beta \left[ \sum_{i=1}^n (y_i - X_i \beta)^2 + \lambda \sum_{j=1}^p |\beta_j| \right]
$$

- Elimina coeficientes irrelevantes
- Modelo mÃ¡s simple e interpretable
- Ideal para selecciÃ³n de variables

---

## ğŸ”§ Entrenamiento y evaluaciÃ³n de Lasso

```{python}
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
y_pred_lasso = lasso.predict(X_test)

r2_lasso = r2_score(y_test, y_pred_lasso)
rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))

r2_lasso, rmse_lasso
```

---

## ğŸ” ValidaciÃ³n Cruzada

Comparamos la capacidad de generalizaciÃ³n de Ridge y Lasso usando 10-fold cross-validation:

```{python}
ridge_cv = cross_val_score(Ridge(alpha=1.0), X, y, cv=10, scoring='r2')
lasso_cv = cross_val_score(Lasso(alpha=0.1), X, y, cv=10, scoring='r2')

ridge_cv.mean(), lasso_cv.mean()
```

---

## ğŸ“Š ComparaciÃ³n de coeficientes

```{python}
plt.figure(figsize=(12,6))
plt.bar(X.columns, ridge.coef_, alpha=0.7, label="Ridge")
plt.bar(X.columns, lasso.coef_, alpha=0.7, label="Lasso")
plt.xticks(rotation=45)
plt.title("Coeficientes: Ridge vs Lasso")
plt.ylabel("Valor del Coeficiente")
plt.legend()
plt.tight_layout()
plt.grid(True)
plt.show()
```

---

## ğŸ“Œ ConclusiÃ³n final de los resultados obtenidos

| Modelo | RÂ² | RMSE | RÂ² CV promedio |
|--------|----|------|----------------|
| Ridge  | `r2_ridge:.2f` | `rmse_ridge:.2f` | `ridge_cv.mean():.2f` |
| Lasso  | `r2_lasso:.2f` | `rmse_lasso:.2f` | `lasso_cv.mean():.2f` |

---

## ğŸ§  InterpretaciÃ³n

- ğŸ“‰ El modelo lineal original mostrÃ³ signos de **sobreajuste**.
- ğŸ› ï¸ Ridge mantuvo todas las variables, redujo varianza y estabilizÃ³ coeficientes.
- ğŸ§¹ Lasso eliminÃ³ coeficientes irrelevantes, simplificando el modelo.
- ğŸ”„ La validaciÃ³n cruzada mostrÃ³ que ambos mÃ©todos **generalizan mejor**.

> âœ… Ridge y Lasso son herramientas clave para construir modelos mÃ¡s robustos y Ãºtiles en producciÃ³n.

---

