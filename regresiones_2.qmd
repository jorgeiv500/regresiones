---
title: "üìâ Regularizaci√≥n con Ridge, Lasso y Validaci√≥n Cruzada"
subtitle: "üî¨ Continuaci√≥n del an√°lisis: Dataset Boston Housing"
author: "üë®‚Äçüè´ Jorge Iv√°n Romero Gelvez"
institute: "üèõÔ∏è Universidad Jorge Tadeo Lozano"
date: "üìÖ Abril 2025"
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
    highlight-style: dracula
    code-line-numbers: true
    code-annotations: hover
    mermaid:
      theme: forest
    transition: fade
    chalkboard: true
    logo: Utadeo70-fondoblanco.png
    toc: true
    toc-title: "Contenido"
    toc-depth: 1
    incremental: true
    scrollable: true
execute:
  warning: false
  message: false
  echo: true
  freeze: false
jupyter: python3
---

## üì• Carga del Dataset

```{python}
import pandas as pd
import numpy as np

# Cargar los datos desde la URL
url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(url, sep="\s+", skiprows=22, header=None)

# Combinar las filas pares e impares en un √∫nico arreglo de datos
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

# Nombres de columnas
columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM',
           'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']

# Crear el DataFrame final
boston_df = pd.DataFrame(np.column_stack([data, target]), columns=columns)

# Divisi√≥n de datos
X = boston_df.drop(columns=['MEDV'])
y = boston_df['MEDV']

n = len(boston_df)
n_train = int(n * 0.8)

X_train = X.iloc[:n_train]
y_train = y.iloc[:n_train]

X_test = X.iloc[n_train:]
y_test = y.iloc[n_train:]
```

## üéØ Objetivo de la Regularizaci√≥n

- ‚ö†Ô∏è El modelo lineal puede sobreajustar
- üß∞ Regularizaci√≥n reduce varianza penalizando la complejidad
- üîé Evaluaremos **Ridge**, **Lasso** y su desempe√±o con **validaci√≥n cruzada**

> **¬øC√≥mo se controla el sobreajuste?**

---

## üîé Regresi√≥n Ridge (L2)

### üßÆ Funci√≥n de costo:

$$
\min_\beta \left( \sum_{i=1}^{n}(y_i - X_i \beta)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right)
$$

- üéØ Penaliza coeficientes grandes
- ‚úÖ Reduce la varianza
- ‚ùó Mantiene todas las variables

---

## üîß Entrenamiento Ridge

```{python}
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
y_pred_ridge = ridge.predict(X_test)
```

---

## üìä Evaluaci√≥n Ridge

```{python}
from sklearn.metrics import mean_squared_error, r2_score

r2_ridge = r2_score(y_test, y_pred_ridge)
rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))

print("R¬≤ Ridge:", r2_ridge)
print("RMSE Ridge:", rmse_ridge)
```

---

## üßπ Regresi√≥n Lasso (L1)

### üßÆ Funci√≥n de costo:

$$
\min_\beta \left( \sum_{i=1}^{n}(y_i - X_i \beta)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right)
$$

- üéØ Penaliza coeficientes absolutos
- üóëÔ∏è Elimina variables irrelevantes
- üß† Modelo m√°s simple e interpretable

---

## üîß Entrenamiento Lasso

```{python}
from sklearn.linear_model import Lasso

lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
y_pred_lasso = lasso.predict(X_test)
```

---

## üìä Evaluaci√≥n Lasso

```{python}
r2_lasso = r2_score(y_test, y_pred_lasso)
rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))

print("R¬≤ Lasso:", r2_lasso)
print("RMSE Lasso:", rmse_lasso)
```

---

## üîÅ Validaci√≥n cruzada Ridge y Lasso

```{python}
ridge_scores = cross_val_score(Ridge(alpha=1.0), X, y, cv=10, scoring='r2')
lasso_scores = cross_val_score(Lasso(alpha=0.1), X, y, cv=10, scoring='r2')

print("Promedio R¬≤ Ridge CV:", ridge_scores.mean())
print("Promedio R¬≤ Lasso CV:", lasso_scores.mean())
```

---

## üìà Comparaci√≥n de Coeficientes

```{python}
import matplotlib.pyplot as plt

plt.figure(figsize=(12,6))
plt.bar(X.columns, ridge.coef_, alpha=0.7, label="Ridge")
plt.bar(X.columns, lasso.coef_, alpha=0.7, label="Lasso")
plt.xticks(rotation=45)
plt.title("Coeficientes: Ridge vs Lasso")
plt.ylabel("Valor")
plt.legend()
plt.tight_layout()
plt.show()
```

---

## ‚úÖ Conclusiones

- üî¨ **Ridge** estabiliza el modelo pero conserva todas las variables
- üßπ **Lasso** selecciona autom√°ticamente, √∫til para modelos interpretables
- üîÑ **Validaci√≥n cruzada** permite estimar desempe√±o real
- üß† La regularizaci√≥n mejora la generalizaci√≥n

> *El equilibrio ideal entre sesgo y varianza requiere an√°lisis matem√°tico y visual.*

