---
<<<<<<< HEAD
title: "ğŸ“‰ RegularizaciÃ³n con Ridge, Lasso y ValidaciÃ³n Cruzada"
subtitle: "ğŸ”¬ ContinuaciÃ³n del anÃ¡lisis: Dataset Boston Housing"
=======
title: "ğŸ“‰ Ridge, Lasso y ValidaciÃ³n Cruzada"
subtitle: "ğŸ”§ Mitigando el sobreajuste en el dataset Boston Housing"
>>>>>>> a8e37dc9e7a77afe4260647f8c1343e86a5e0480
author: "ğŸ‘¨â€ğŸ« Jorge IvÃ¡n Romero Gelvez"
institute: "ğŸ›ï¸ Universidad Jorge Tadeo Lozano"
date: "ğŸ“… Abril 2025"
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
    highlight-style: dracula
    code-line-numbers: true
    code-annotations: hover
<<<<<<< HEAD
    mermaid:
      theme: forest
    transition: fade
    chalkboard: true
    logo: Utadeo70-fondoblanco.png
=======
    transition: fade
>>>>>>> a8e37dc9e7a77afe4260647f8c1343e86a5e0480
    toc: true
    toc-title: "Contenido"
    toc-depth: 1
    incremental: true
    scrollable: true
execute:
<<<<<<< HEAD
  warning: false
  message: false
  echo: true
=======
  echo: true
  warning: false
  message: false
>>>>>>> a8e37dc9e7a77afe4260647f8c1343e86a5e0480
  freeze: false
jupyter: python3
---

<<<<<<< HEAD
## ğŸ“¥ Carga del Dataset
=======
## âš™ï¸ PreparaciÃ³n del entorno
>>>>>>> a8e37dc9e7a77afe4260647f8c1343e86a5e0480

```{python}
import pandas as pd
import numpy as np
<<<<<<< HEAD

# Cargar los datos desde la URL
url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(url, sep="\s+", skiprows=22, header=None)

# Combinar las filas pares e impares en un Ãºnico arreglo de datos
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

# Nombres de columnas
columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM',
           'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']

# Crear el DataFrame final
boston_df = pd.DataFrame(np.column_stack([data, target]), columns=columns)

# DivisiÃ³n de datos
X = boston_df.drop(columns=['MEDV'])
y = boston_df['MEDV']

n = len(boston_df)
n_train = int(n * 0.8)

X_train = X.iloc[:n_train]
y_train = y.iloc[:n_train]

X_test = X.iloc[n_train:]
y_test = y.iloc[n_train:]
```

## ğŸ¯ Objetivo de la RegularizaciÃ³n

- âš ï¸ El modelo lineal puede sobreajustar
- ğŸ§° RegularizaciÃ³n reduce varianza penalizando la complejidad
- ğŸ” Evaluaremos **Ridge**, **Lasso** y su desempeÃ±o con **validaciÃ³n cruzada**

> **Â¿CÃ³mo se controla el sobreajuste?**

---

## ğŸ” RegresiÃ³n Ridge (L2)

### ğŸ§® FunciÃ³n de costo:

$$
\min_\beta \left( \sum_{i=1}^{n}(y_i - X_i \beta)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right)
$$

- ğŸ¯ Penaliza coeficientes grandes
- âœ… Reduce la varianza
- â— Mantiene todas las variables

---

## ğŸ”§ Entrenamiento Ridge

```{python}
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
y_pred_ridge = ridge.predict(X_test)
```

---

## ğŸ“Š EvaluaciÃ³n Ridge

```{python}
from sklearn.metrics import mean_squared_error, r2_score
=======
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt

# Cargar dataset Boston Housing
url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM',
           'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']

X = pd.DataFrame(data, columns=columns)
y = target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

---

## ğŸ§® RegresiÃ³n Ridge

La regresiÃ³n Ridge aÃ±ade una penalizaciÃ³n L2 a la funciÃ³n de costo:

$$
\min_\beta \left[ \sum_{i=1}^n (y_i - X_i \beta)^2 + \lambda \sum_{j=1}^p \beta_j^2 \right]
$$

- Reduce coeficientes grandes
- Mantiene todas las variables
- Ideal para multicolinealidad

---

## ğŸ”§ Entrenamiento y evaluaciÃ³n de Ridge

```{python}
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
y_pred_ridge = ridge.predict(X_test)
>>>>>>> a8e37dc9e7a77afe4260647f8c1343e86a5e0480

r2_ridge = r2_score(y_test, y_pred_ridge)
rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))

<<<<<<< HEAD
print("RÂ² Ridge:", r2_ridge)
print("RMSE Ridge:", rmse_ridge)
=======
r2_ridge, rmse_ridge
>>>>>>> a8e37dc9e7a77afe4260647f8c1343e86a5e0480
```

---

<<<<<<< HEAD
## ğŸ§¹ RegresiÃ³n Lasso (L1)

### ğŸ§® FunciÃ³n de costo:

$$
\min_\beta \left( \sum_{i=1}^{n}(y_i - X_i \beta)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right)
$$

- ğŸ¯ Penaliza coeficientes absolutos
- ğŸ—‘ï¸ Elimina variables irrelevantes
- ğŸ§  Modelo mÃ¡s simple e interpretable

---

## ğŸ”§ Entrenamiento Lasso

```{python}
from sklearn.linear_model import Lasso

lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
y_pred_lasso = lasso.predict(X_test)
```

---

## ğŸ“Š EvaluaciÃ³n Lasso

```{python}
r2_lasso = r2_score(y_test, y_pred_lasso)
rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))

print("RÂ² Lasso:", r2_lasso)
print("RMSE Lasso:", rmse_lasso)
=======
## ğŸ§¹ RegresiÃ³n Lasso

La regresiÃ³n Lasso aÃ±ade penalizaciÃ³n L1:

$$
\min_\beta \left[ \sum_{i=1}^n (y_i - X_i \beta)^2 + \lambda \sum_{j=1}^p |\beta_j| \right]
$$

- Elimina coeficientes irrelevantes
- Modelo mÃ¡s simple e interpretable
- Ideal para selecciÃ³n de variables

---

## ğŸ”§ Entrenamiento y evaluaciÃ³n de Lasso

```{python}
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
y_pred_lasso = lasso.predict(X_test)

r2_lasso = r2_score(y_test, y_pred_lasso)
rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))

r2_lasso, rmse_lasso
>>>>>>> a8e37dc9e7a77afe4260647f8c1343e86a5e0480
```

---

<<<<<<< HEAD
## ğŸ” ValidaciÃ³n cruzada Ridge y Lasso

```{python}
ridge_scores = cross_val_score(Ridge(alpha=1.0), X, y, cv=10, scoring='r2')
lasso_scores = cross_val_score(Lasso(alpha=0.1), X, y, cv=10, scoring='r2')

print("Promedio RÂ² Ridge CV:", ridge_scores.mean())
print("Promedio RÂ² Lasso CV:", lasso_scores.mean())
=======
## ğŸ” ValidaciÃ³n Cruzada

Comparamos la capacidad de generalizaciÃ³n de Ridge y Lasso usando 10-fold cross-validation:

```{python}
ridge_cv = cross_val_score(Ridge(alpha=1.0), X, y, cv=10, scoring='r2')
lasso_cv = cross_val_score(Lasso(alpha=0.1), X, y, cv=10, scoring='r2')

ridge_cv.mean(), lasso_cv.mean()
>>>>>>> a8e37dc9e7a77afe4260647f8c1343e86a5e0480
```

---

<<<<<<< HEAD
## ğŸ“ˆ ComparaciÃ³n de Coeficientes

```{python}
import matplotlib.pyplot as plt

=======
## ğŸ“Š ComparaciÃ³n de coeficientes

```{python}
>>>>>>> a8e37dc9e7a77afe4260647f8c1343e86a5e0480
plt.figure(figsize=(12,6))
plt.bar(X.columns, ridge.coef_, alpha=0.7, label="Ridge")
plt.bar(X.columns, lasso.coef_, alpha=0.7, label="Lasso")
plt.xticks(rotation=45)
plt.title("Coeficientes: Ridge vs Lasso")
<<<<<<< HEAD
plt.ylabel("Valor")
plt.legend()
plt.tight_layout()
=======
plt.ylabel("Valor del Coeficiente")
plt.legend()
plt.tight_layout()
plt.grid(True)
>>>>>>> a8e37dc9e7a77afe4260647f8c1343e86a5e0480
plt.show()
```

---

<<<<<<< HEAD
## âœ… Conclusiones

- ğŸ”¬ **Ridge** estabiliza el modelo pero conserva todas las variables
- ğŸ§¹ **Lasso** selecciona automÃ¡ticamente, Ãºtil para modelos interpretables
- ğŸ”„ **ValidaciÃ³n cruzada** permite estimar desempeÃ±o real
- ğŸ§  La regularizaciÃ³n mejora la generalizaciÃ³n

> *El equilibrio ideal entre sesgo y varianza requiere anÃ¡lisis matemÃ¡tico y visual.*
=======
## ğŸ“Œ ConclusiÃ³n final de los resultados obtenidos

```{python}
import pandas as pd

# Suponiendo que ya tienes calculados estos valores:
# r2_ridge, rmse_ridge, ridge_cv
# r2_lasso, rmse_lasso, lasso_cv

# Crear el DataFrame resumen
resultados_df = pd.DataFrame({
    'Modelo': ['Ridge', 'Lasso'],
    'RÂ²': [r2_ridge, r2_lasso],
    'RMSE': [rmse_ridge, rmse_lasso],
    'RÂ² CV promedio': [ridge_cv.mean(), lasso_cv.mean()]
})

# Mostrar el DataFrame con dos decimales
resultados_df = resultados_df.round(2)
resultados_df
```

---

## ğŸ§  InterpretaciÃ³n

- ğŸ“‰ El modelo lineal original mostrÃ³ signos de **sobreajuste**.
- ğŸ› ï¸ Ridge mantuvo todas las variables, redujo varianza y estabilizÃ³ coeficientes.
- ğŸ§¹ Lasso eliminÃ³ coeficientes irrelevantes, simplificando el modelo.
- ğŸ”„ La validaciÃ³n cruzada mostrÃ³ que ambos mÃ©todos **generalizan mejor**.

> âœ… Ridge y Lasso son herramientas clave para construir modelos mÃ¡s robustos y Ãºtiles en producciÃ³n.

---

## ğŸ“˜  NÃºmeros y estructuras matemÃ¡ticas

## ğŸ”¹ Escalares
- Un **escalar** es un Ãºnico nÃºmero.
- Representa una cantidad en una dimensiÃ³n.
- Ejemplo: `x = 5`

## ğŸ”¹ Vectores
- Lista ordenada de nÃºmeros (componentes).
- DirecciÃ³n y magnitud:

$\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \dots \\ x_n \end{bmatrix}$

## ğŸ”¹ Matrices
- Tabla bidimensional de nÃºmeros:

$ A = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}$

## ğŸ”¹ Tensors
- GeneralizaciÃ³n a mÃºltiples dimensiones.
- Ej. imÃ¡genes RGB, secuencias, videos, etc.

---

# ğŸ“˜ Probabilidad y distribuciones

## ğŸ”¹ Fundamentos
- **Probabilidad** mide incertidumbre.
- **Variable aleatoria**: asigna valores a eventos.

## ğŸ”¹ Distribuciones comunes
- **Bernoulli**, **Binomial**, **Normal**:


$\mathcal{N}(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x - \mu)^2}{2\sigma^2} \right)$


![Distribuciones comunes](distr.png)


---

# ğŸ“˜ TeorÃ­a de la informaciÃ³n

## ğŸ”¹ EntropÃ­a

$H(X) = -\sum_{x} P(x) \log P(x)$

## ğŸ”¹ EntropÃ­a cruzada

$H(P, Q) = -\sum_x P(x) \log Q(x)$

## ğŸ”¹ Divergencia KL

$D_{KL}(P \| Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
$

## ğŸ”¹ InformaciÃ³n mutua

$I(X; Y) = H(X) - H(X | Y)
$

---

# ğŸ“˜ ParÃ¡metros y optimizaciÃ³n

## ğŸ”¹ ParÃ¡metros vs HiperparÃ¡metros
- **ParÃ¡metros**: aprendidos durante entrenamiento (ej. pesos  $W$ ).
- **HiperparÃ¡metros**: definidos antes (ej. tasa de aprendizaje). Ejemplo, en ridge y lasso, $\lambda$

## ğŸ”¹ OptimizaciÃ³n
- Minimizar la funciÃ³n de pÃ©rdida.

### MÃ©todos:
- **Gradiente descendiente**:

$
\theta \leftarrow \theta - \eta \nabla_\theta J(\theta)
$

- **SGD**, **Adam**, **RMSProp**

---

# Â¡Gracias por tu atenciÃ³n! ğŸ™Œ

- Material basado en *Deep Learning* â€” Goodfellow, Bengio y Courville.
- PresentaciÃ³n para uso docente.

---

>>>>>>> a8e37dc9e7a77afe4260647f8c1343e86a5e0480

