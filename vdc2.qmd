---
title: "Â¿CÃ³mo aprende una IA a jugar Snake?"
author: "Jorge IvÃ¡n Romero Gelvez"
format:
  revealjs:
    theme: custom.scss
    transition: fade
    slide-number: true
    toc: false
    chalkboard: true
    logo: "https://www.utadeo.edu.co/sites/tadeo/files/logoutadeo_1.jpg"
    footer: "ğŸ® Â¡Viernes de CÃ³digo en Utadeo!"
---

# ğŸ Â¿CÃ³mo aprende una IA a jugar Snake?
<br>
<div style="text-align:center;">
<img src="https://art.pixilart.com/76cecf6b86ae119.gif" alt="Snake Game" width="300"/>
</div>
<br>
> El modelo de IA aprende **jugando contra sÃ­ mismo** y usando su experiencia para mejorar.

---

## Â¿QuÃ© "ve" la IA? ğŸ§

**Estado (State):**  
La IA recibe un vector de **11 valores booleanos** cada vez que toma una decisiÃ³n.

<div class="fragment">
<ul>
  <li><b>Peligro</b>: Â¿Hay obstÃ¡culo adelante, a la derecha o a la izquierda?</li>
  <li><b>DirecciÃ³n actual</b>: Â¿Va hacia arriba, abajo, izquierda o derecha?</li>
  <li><b>Comida</b>: Â¿DÃ³nde estÃ¡ la comida respecto a la cabeza de la serpiente?</li>
</ul>
</div>

<div class="fragment" style="text-align:center;">
<img src="https://www.researchgate.net/profile/Silvia-Ullo/publication/351884746/figure/fig1/AS:1067993664589824@1631640940747/Q-Learning-vs-Deep-Q-Learning.ppm" alt="snake state" width="400"/>
</div>

---

## Ejemplo de vector de estado

```{python}
#| eval: false
estado = [0, 1, 0,   0, 1, 0, 0,   1, 0, 0, 0]
#   peligro     direcciÃ³n     comida
```

<div style="font-size:1.2em;">
<p>ğŸŸ¥ Peligro adelante</p>
<p>ğŸŸ© DirecciÃ³n actual: derecha</p>
<p>ğŸ Comida a la izquierda</p>
</div>

---

## Â¿CÃ³mo decide la IA? ğŸ¤”

**Acciones posibles:**

* <span style="color:green;">Seguir recto</span> ğŸŸ¢
* <span style="color:orange;">Girar a la derecha</span> ğŸŸ 
* <span style="color:blue;">Girar a la izquierda</span> ğŸ”µ

<div style="text-align:center;">
<img src="https://www.researchgate.net/profile/Silvia-Ullo/publication/351884746/figure/fig4/AS:1067993668804609@1631640941629/Deep-Neural-Network-architecture.ppm" alt="acciones snake" width="720"/>
</div>

<aside class="notes">
La decisiÃ³n es siempre relativa a la direcciÃ³n actual para evitar giros de 180Â°.
</aside>

---

## Â¿CÃ³mo sabe si lo estÃ¡ haciendo bien? ğŸ†

**Sistema de recompensas:**

| Evento                          | Recompensa |
| ------------------------------- | :--------: |
| Come una comida                 |     +10    |
| Muere (colisiÃ³n)                |     -10    |
| No progresa (demasiados turnos) |     -10    |
| Otro paso (sin evento especial) |      0     |

<div style="text-align:center;">
<img src="https://www.researchgate.net/profile/Silvia-Ullo/publication/351884746/figure/fig5/AS:1067993668780035@1631640941758/The-Snake-game-GUI.ppm" alt="snake food" width="150"/>

</div>

---

## Â¿CÃ³mo aprende? ğŸ“š

* Cada experiencia `(estado, acciÃ³n, recompensa, nuevo_estado, fin_de_juego)`
  se **almacena en una "memoria"** (tipo `deque`).
* El modelo **repite partidas y almacena hasta 100,000 experiencias**.

<div class="fragment" style="text-align:center;">
<img src="https://media1.tenor.com/m/7HUogy7rXs4AAAAC/feel-me-think-about-it.gif" alt="replay memory" width="400"/>
</div>

---

## La red neuronal (LinearQNet)

* **Entradas:** Los 11 valores del estado
* **Capas ocultas:** (Por ejemplo, 2 capas de 256 neuronas)
* **Salidas:** 3 acciones posibles (recto, derecha, izquierda)

```{python}
#| eval: false
class LinearQNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(11, 256)
        self.linear2 = nn.Linear(256, 256)
        self.linear3 = nn.Linear(256, 3)
    def forward(self, x):
        x = F.relu(self.linear1(x))
        x = F.relu(self.linear2(x))
        return self.linear3(x)
```

---

## Â¿CÃ³mo se entrena la red? ğŸ› ï¸

* Se seleccionan al azar lotes de experiencias de la memoria (â€œreproducciÃ³n de experienciaâ€)
* La red ajusta sus pesos usando el **Error CuadrÃ¡tico Medio (MSE)** y la ecuaciÃ³n de Bellman para actualizar los valores Q.

<div style="text-align:center;">
<img src="train.png" alt="Red neuronal" width="800"/>
</div>

---

## Resultados del entrenamiento ğŸ“ˆ

* La IA comienza moviÃ©ndose al azar.
* Poco a poco, sobrevive mÃ¡s, come mÃ¡s comida y **aprende estrategias**.
* Â¡Todo gracias a la experiencia acumulada y el entrenamiento iterativo!

<div style="text-align:center;">
<img src="train.png" alt="snake learning curve" width="800"/>
</div>

---

## Preguntas y discusiÃ³n

* Â¿QuÃ© mejoras le harÃ­as al modelo?
* Â¿CÃ³mo cambiarÃ­a el comportamiento si le damos mÃ¡s o menos memoria?
* Â¿QuÃ© otros juegos simples pueden ser usados para enseÃ±ar IA?

---

## Recursos

* [Repositorio original - IA Snake con PyTorch](https://github.com/python-engineer/snake-ai-pytorch)
* [TensorFlow Playground](https://playground.tensorflow.org/)
* [NN-SVG](https://alexlenail.me/NN-SVG/index.html)

---

## Â¡Gracias!

Â¿Listo para programar tu propio agente Snake?

<div style="text-align:center;">
<img src="https://upload.wikimedia.org/wikipedia/commons/3/36/Snake_game.gif" alt="Snake Game" width="200"/>
</div>
---
